{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate LI\n",
    "This notebook calcualtes the lifted index values for us to train models with. See the README to learn what the Lifted Index is. It reads in the raw data-por IGRA data. It then unpivots the data, converting columns back into pressure levels. This data can then be used by MetPy to calculate the Lifted Index using the actual physics algorithms. A CSV file is geratated for each input file.\n",
    "\n",
    "**This notebook will take a long time to complete!**\n",
    "\n",
    "Update the following parameters in the first cell to accomodate your installation:\n",
    "\n",
    "- SILVER_GPH20S10K_PATH - The location to read the transformed CSV files\n",
    "- SILVER_LI_PATH - Location to save the calculated Lifted Index values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from metpy.calc import parcel_profile, lifted_index\n",
    "from metpy.units import units\n",
    "import olieigra\n",
    "\n",
    "BRONZE_DATA_POR_PATH = '/Users/olievortex/lakehouse/default/Files/bronze/igra2/data-por'\n",
    "SILVER_LI_PATH = '/Users/olievortex/lakehouse/default/Files/silver/igra2/li'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need to install olieigra, uncomment and execute this line. View the README in the project root for instructions\n",
    "# on how to build or download this file.\n",
    "#%pip install /lakehouse/default/Files/libs/olieigra-0.0.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the destination folder exists\n",
    "os.makedirs(SILVER_LI_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "class LiftedIndex(olieigra.Callbacks):\n",
    "    \"\"\"Calculate the lifted index from data-por igra data\"\"\"\n",
    "\n",
    "    def __init__(self, dst_path: str, min_effective_date: datetime):\n",
    "        super().__init__()\n",
    "        self.dst_path = dst_path\n",
    "        self.min_effective_date = min_effective_date\n",
    "        self.filtered = 0\n",
    "        self.errors = 0\n",
    "        self.data = 0\n",
    "        self.hout = ''\n",
    "        self.filename = ''\n",
    "        self.writer = None\n",
    "\n",
    "    def start_file(self, filename: str) -> bool:\n",
    "        \"\"\"Decide if we want to process the file. If so, reset state and start writing to a\n",
    "        temporary file.\"\"\"\n",
    "\n",
    "        # An IGRA2 file should end with -data.txt\n",
    "        if not filename.endswith('-data.txt'):\n",
    "            print(f'Skipping {filename}. Not sure what to do with it.')\n",
    "            return False\n",
    "\n",
    "        # Set the desired destination filename\n",
    "        dst_filename = f'{self.dst_path}/{filename}'\n",
    "        dst_filename = dst_filename.replace(\"-data.txt\", \"-data-li.csv\")\n",
    "\n",
    "        # Skip this file if it has already been processed\n",
    "        if os.path.exists(dst_filename):\n",
    "            print(f'Skipping {filename}. Destination file already exists.')\n",
    "            return False\n",
    "\n",
    "        # If we got here, we are going to process the file\n",
    "        print(f'Processing {filename}.')\n",
    "\n",
    "        # Write to a temp file\n",
    "        self.filename = dst_filename.replace('-data-li.csv', '-data-li.partial.csv')\n",
    "        self.writer = open(self.filename, 'w', encoding='UTF-8')\n",
    "\n",
    "        # Reset the filtered record count\n",
    "        self.filtered = 0\n",
    "        self.errors = 0\n",
    "        self.data = 0\n",
    "        self.hout = ''\n",
    "\n",
    "        # Write the header row\n",
    "        self.writer.write('id,effective_date,hour,li\\n')\n",
    "\n",
    "        # Tell olieigra to continue processing\n",
    "        return True\n",
    "\n",
    "    def finish_file(self, headers: int, rows: int):\n",
    "        \"\"\"File processing is complete. Clean up and provide user feedback.\"\"\"\n",
    "\n",
    "        # Close the temporary file\n",
    "        self.writer.close()\n",
    "\n",
    "        # Rename the temporary file\n",
    "        dst_renamed = self.filename.replace('.partial.csv', '.csv')\n",
    "        os.rename(self.filename, dst_renamed)\n",
    "\n",
    "        # Calculate the number of records written\n",
    "        loaded = headers - self.filtered - self.errors - self.data\n",
    "\n",
    "        # Provide feedback to the user\n",
    "        print(f\" Read {headers} headers, {rows} lines. Filtered {self.filtered}. \" +\n",
    "              f\"Errors {self.errors}. Bad data {self.data}. Wrote {loaded} records.\")\n",
    "\n",
    "    def parse_header(self, header: olieigra.HeaderModel) -> bool:\n",
    "        \"\"\"Transform the header record and start writing a record\"\"\"\n",
    "\n",
    "        # Combine the separate fields into a date\n",
    "        effective_date = datetime(header.year, header.month, header.day)\n",
    "\n",
    "        # Skip the record if it is too old\n",
    "        if effective_date < self.min_effective_date:\n",
    "            self.filtered += 1\n",
    "            return False\n",
    "\n",
    "        # We may not write the row, so save the header columns in a variable for now\n",
    "        self.hout = f'{header.id},{effective_date:%Y-%m-%d},{header.hour}'\n",
    "\n",
    "        # Tell olieigra to process the body associated with this header\n",
    "        return True\n",
    "    \n",
    "    def parse_body(self, body: list[olieigra.BodyModel]):\n",
    "        \"\"\"Perform some analytics and finish writing a record\"\"\"\n",
    "        \n",
    "        # Initialize\n",
    "        usable_all = 0\n",
    "        surface_nan = 1\n",
    "        pres = []\n",
    "        temp = []\n",
    "        dp = []\n",
    "\n",
    "        # Iterate through each record in the body\n",
    "        for item in body:\n",
    "            # We don't care about non-pressure records\n",
    "            if item.type[0] == '3':\n",
    "                continue\n",
    "\n",
    "            # We don't want records that contain a NaN value\n",
    "            if math.isnan(item.dpdp) | math.isnan(item.temp):\n",
    "                continue\n",
    "\n",
    "            # This is a usable record\n",
    "            usable_all += 1\n",
    "\n",
    "            # Flag that we found a valid surface record\n",
    "            if item.type == '21':\n",
    "                surface_nan = 0\n",
    "\n",
    "            pres.append(item.pres / 100.0)\n",
    "            temp.append(item.temp / 10.0)\n",
    "            dp.append((item.temp - item.dpdp) / 10.0)\n",
    "\n",
    "        # Quality checks\n",
    "        if surface_nan == 1 or len(pres) < 20:\n",
    "            self.data += 1\n",
    "            return\n",
    "        \n",
    "        # Calulate lifted index\n",
    "        li = self.calculate_li(pres, temp, dp)\n",
    "        \n",
    "        # If LI is NaN, there was an exception in the calulation\n",
    "        if math.isnan(li):\n",
    "            self.errors += 1\n",
    "            return\n",
    "        \n",
    "        # Write record\n",
    "        if surface_nan == 0:\n",
    "            self.writer.write(f'{self.hout},{li:.1f}\\n')\n",
    "\n",
    "    def calculate_li(self, pres: list[float], temp: list[float], dp: list[float]) -> float:\n",
    "        \"\"\"Calculate the lifted index using metpy\"\"\"\n",
    "\n",
    "        # Conver the units\n",
    "        pres = np.array(pres) * units.hPa\n",
    "        temp = np.array(temp) * units.degC\n",
    "        dp = np.array(dp) * units.degC\n",
    "\n",
    "        # Calculate parameters\n",
    "        try:\n",
    "            # Calculate the parcel profile\n",
    "            prof = parcel_profile(pres, temp[0], dp[0])\n",
    "\n",
    "            # The calculation throws an exception if the data doesn't reach the EL\n",
    "            li = lifted_index(pres, temp, prof)\n",
    "\n",
    "            return float(li.magnitude[0])\n",
    "        except:\n",
    "            return math.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing USM00072251-data.txt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7x/y_0m3dbn3pl7shgklj8r717m0000gn/T/ipykernel_3660/3984590618.py:149: UserWarning: Interpolation point out of data bounds encountered\n",
      "  li = lifted_index(pres, temp, prof)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Read 34081 headers, 4532303 lines. Filtered 16310. Errors 33. Bad data 1350. Wrote 16388 records.\n",
      "Processing USM00072357-data.txt.\n",
      " Read 26409 headers, 4084416 lines. Filtered 8706. Errors 44. Bad data 1025. Wrote 16634 records.\n",
      "Processing USM00072250-data.txt.\n",
      " Read 77417 headers, 5455633 lines. Filtered 59498. Errors 18. Bad data 1537. Wrote 16364 records.\n",
      "Processing USM00072456-data.txt.\n",
      " Read 54326 headers, 5136424 lines. Filtered 36402. Errors 38. Bad data 945. Wrote 16941 records.\n",
      "Processing USM00072451-data.txt.\n",
      " Read 65711 headers, 4849890 lines. Filtered 47852. Errors 23. Bad data 1106. Wrote 16730 records.\n",
      "Processing USM00072649-data.txt.\n",
      " Read 26612 headers, 3659546 lines. Filtered 9224. Errors 25. Bad data 946. Wrote 16417 records.\n",
      "Processing USM00072261-data.txt.\n",
      " Read 53682 headers, 5133602 lines. Filtered 35986. Errors 86. Bad data 1479. Wrote 16131 records.\n",
      "Processing USM00074560-data.txt.\n",
      " Read 21214 headers, 3761759 lines. Filtered 3539. Errors 87. Bad data 793. Wrote 16795 records.\n",
      "Processing USM00072645-data.txt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/igra/lib/python3.11/site-packages/metpy/calc/thermo.py:1313: RuntimeWarning: overflow encountered in exp\n",
      "  return mpconsts.nounit.sat_pressure_0c * np.exp(\n",
      "/opt/miniconda3/envs/igra/lib/python3.11/site-packages/metpy/calc/thermo.py:1458: RuntimeWarning: invalid value encountered in divide\n",
      "  return molecular_weight_ratio * partial_press / (total_press - partial_press)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Read 61708 headers, 4823579 lines. Filtered 44399. Errors 18. Bad data 832. Wrote 16459 records.\n",
      "Processing USM00072249-data.txt.\n",
      " Read 49737 headers, 4446304 lines. Filtered 31519. Errors 30. Bad data 627. Wrote 17561 records.\n",
      "Processing USM00072747-data.txt.\n",
      " Read 61276 headers, 4818934 lines. Filtered 44039. Errors 46. Bad data 660. Wrote 16531 records.\n",
      "Processing USM00072662-data.txt.\n",
      " Read 69285 headers, 4949468 lines. Filtered 51686. Errors 18. Bad data 1208. Wrote 16373 records.\n",
      "Processing USM00072558-data.txt.\n",
      " Read 83354 headers, 5105789 lines. Filtered 65841. Errors 39. Bad data 1343. Wrote 16131 records.\n",
      "Processing USM00074455-data.txt.\n",
      " Read 23193 headers, 3857507 lines. Filtered 5560. Errors 57. Bad data 536. Wrote 17040 records.\n",
      "Processing USM00072764-data.txt.\n",
      " Read 78952 headers, 5027576 lines. Filtered 61574. Errors 12. Bad data 1031. Wrote 16335 records.\n",
      "Processing USM00072659-data.txt.\n",
      " Read 21198 headers, 3718703 lines. Filtered 3693. Errors 22. Bad data 845. Wrote 16638 records.\n",
      "Processing USM00074646-data.txt.\n",
      " Read 25608 headers, 2377189 lines. Filtered 0. Errors 8. Bad data 1677. Wrote 23923 records.\n",
      "Processing USM00072363-data.txt.\n",
      " Read 76459 headers, 4914828 lines. Filtered 59076. Errors 13. Bad data 1142. Wrote 16228 records.\n",
      "Processing USM00072364-data.txt.\n",
      " Read 75625 headers, 5000396 lines. Filtered 58112. Errors 4. Bad data 2046. Wrote 15463 records.\n",
      "Processing USM00072440-data.txt.\n",
      " Read 32343 headers, 3866497 lines. Filtered 14599. Errors 59. Bad data 551. Wrote 17134 records.\n",
      "Processing USM00072562-data.txt.\n",
      " Read 76207 headers, 5081951 lines. Filtered 58738. Errors 19. Bad data 1039. Wrote 16411 records.\n",
      "Processing USM00072265-data.txt.\n",
      " Read 62424 headers, 4934369 lines. Filtered 44950. Errors 13. Bad data 1711. Wrote 15750 records.\n",
      "Processing USM00072476-data.txt.\n",
      " Read 68763 headers, 4779304 lines. Filtered 51675. Errors 4. Bad data 1664. Wrote 15420 records.\n"
     ]
    }
   ],
   "source": [
    "# Set up for processing\n",
    "callbacks = LiftedIndex(SILVER_LI_PATH, datetime(2000, 1, 1))\n",
    "reader = olieigra.Reader(callbacks=callbacks)\n",
    "crawler = olieigra.Crawler(reader=reader)\n",
    "\n",
    "# Crawl and process files\n",
    "crawler.crawl(BRONZE_DATA_POR_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olieigra_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
