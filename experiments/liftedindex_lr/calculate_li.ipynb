{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate LI\n",
    "This notebook calcualtes the lifted index values for us to train models with. See the README to learn what the Lifted Index is. It reads in the raw data-por IGRA data. It then unpivots the data, converting columns back into pressure levels. This data can then be used by MetPy to calculate the Lifted Index using the actual physics algorithms. A CSV file is geratated for each input file.\n",
    "\n",
    "**This notebook will take a long time to complete!**\n",
    "\n",
    "Update the following parameters in the first cell to accomodate your installation:\n",
    "\n",
    "- SILVER_GPH20S10K_PATH - The location to read the transformed CSV files\n",
    "- SILVER_LI_PATH - Location to save the calculated Lifted Index values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from metpy.calc import parcel_profile, lifted_index\n",
    "from metpy.units import units\n",
    "import olieigra\n",
    "\n",
    "BRONZE_DATA_POR_PATH = '/lakehouse/default/Files/bronze/igra2/data-por'\n",
    "SILVER_LI_PATH = '/lakehouse/default/Files/silver/igra2/li'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need to install olieigra, uncomment and execute this line. View the README in the project root for instructions\n",
    "# on how to build or download this file.\n",
    "#%pip install /lakehouse/default/Files/libs/olieigra-0.0.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the destination folder exists\n",
    "os.makedirs(SILVER_LI_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "class LiftedIndex(olieigra.Callbacks):\n",
    "    \"\"\"Calculate the lifted index from data-por igra data\"\"\"\n",
    "\n",
    "    def __init__(self, dst_path: str, min_effective_date: datetime):\n",
    "        super().__init__()\n",
    "        self.dst_path = dst_path\n",
    "        self.min_effective_date = min_effective_date\n",
    "        self.filtered = 0\n",
    "        self.errors = 0\n",
    "        self.data = 0\n",
    "        self.hout = ''\n",
    "        self.filename = ''\n",
    "        self.writer = None\n",
    "\n",
    "    def start_file(self, filename: str) -> bool:\n",
    "        \"\"\"Decide if we want to process the file. If so, reset state and start writing to a\n",
    "        temporary file.\"\"\"\n",
    "\n",
    "        # An IGRA2 file should end with -data.txt\n",
    "        if not filename.endswith('-data.txt'):\n",
    "            print(f'Skipping {filename}. Not sure what to do with it.')\n",
    "            return False\n",
    "\n",
    "        # Set the desired destination filename\n",
    "        dst_filename = f'{self.dst_path}/{filename}'\n",
    "        dst_filename = dst_filename.replace(\"-data.txt\", \"-data-li.csv\")\n",
    "\n",
    "        # Skip this file if it has already been processed\n",
    "        if os.path.exists(dst_filename):\n",
    "            print(f'Skipping {filename}. Destination file already exists.')\n",
    "            return False\n",
    "\n",
    "        # If we got here, we are going to process the file\n",
    "        print(f'Processing {filename}.')\n",
    "\n",
    "        # Write to a temp file\n",
    "        self.filename = dst_filename.replace('-data-li.csv', '-data-li.partial.csv')\n",
    "        self.writer = open(self.filename, 'w', encoding='UTF-8')\n",
    "\n",
    "        # Reset the filtered record count\n",
    "        self.filtered = 0\n",
    "        self.errors = 0\n",
    "        self.hout = ''\n",
    "\n",
    "        # Write the header row\n",
    "        self.writer.write('id,effective_date,hour,li\\n')\n",
    "\n",
    "        # Tell olieigra to continue processing\n",
    "        return True\n",
    "\n",
    "    def finish_file(self, headers: int, rows: int):\n",
    "        \"\"\"File processing is complete. Clean up and provide user feedback.\"\"\"\n",
    "\n",
    "        # Close the temporary file\n",
    "        self.writer.close()\n",
    "\n",
    "        # Rename the temporary file\n",
    "        dst_renamed = self.filename.replace('.partial.csv', '.csv')\n",
    "        os.rename(self.filename, dst_renamed)\n",
    "\n",
    "        # Calculate the number of records written\n",
    "        loaded = headers - self.filtered - self.errors - self.data\n",
    "\n",
    "        # Provide feedback to the user\n",
    "        print(f\" Read {headers} headers, {rows} lines. Filtered {self.filtered}. \" +\n",
    "              f\"Errors {self.errors}. Bad data {self.data}. Wrote {loaded} records.\")\n",
    "\n",
    "    def parse_header(self, header: olieigra.HeaderModel) -> bool:\n",
    "        \"\"\"Transform the header record and start writing a record\"\"\"\n",
    "\n",
    "        # Combine the separate fields into a date\n",
    "        effective_date = datetime(header.year, header.month, header.day)\n",
    "\n",
    "        # Skip the record if it is too old\n",
    "        if effective_date < self.min_effective_date:\n",
    "            self.filtered += 1\n",
    "            return False\n",
    "\n",
    "        # We may not write the row, so save the header columns in a variable for now\n",
    "        self.hout = f'{header.id},{effective_date:%Y-%m-%d},{header.hour}'\n",
    "\n",
    "        # Tell olieigra to process the body associated with this header\n",
    "        return True\n",
    "    \n",
    "    def parse_body(self, body: list[olieigra.BodyModel]):\n",
    "        \"\"\"Perform some analytics and finish writing a record\"\"\"\n",
    "        \n",
    "        # Initialize\n",
    "        usable_all = 0\n",
    "        surface_nan = 1\n",
    "        pres = []\n",
    "        temp = []\n",
    "        dp = []\n",
    "\n",
    "        # Iterate through each record in the body\n",
    "        for item in body:\n",
    "            # We don't care about non-pressure records\n",
    "            if item.type[0] == '3':\n",
    "                continue\n",
    "\n",
    "            # We don't want records that contain a NaN value\n",
    "            if math.isnan(item.dpdp) | math.isnan(item.temp):\n",
    "                continue\n",
    "\n",
    "            # This is a usable record\n",
    "            usable_all += 1\n",
    "\n",
    "            # Flag that we found a valid surface record\n",
    "            if item.type == '21':\n",
    "                surface_nan = 0\n",
    "\n",
    "            pres.append(item.pres / 100.0)\n",
    "            temp.append(item.temp / 10.0)\n",
    "            dp.append((item.temp - item.dpdp) / 10.0)\n",
    "\n",
    "        # Quality checks\n",
    "        if surface_nan == 1 or len(pres) < 20:\n",
    "            self.data += 1\n",
    "            return\n",
    "        \n",
    "        # Calulate lifted index\n",
    "        li = self.calculate_li(pres, temp, dp)\n",
    "        \n",
    "        # If LI is NaN, there was an exception in the calulation\n",
    "        if math.isnan(li):\n",
    "            self.errors += 1\n",
    "            return\n",
    "        \n",
    "        # Write record\n",
    "        if surface_nan == 0:\n",
    "            self.writer.write(f'{self.hout},{li:.1f}\\n')\n",
    "\n",
    "    def calculate_li(self, pres: list[float], temp: list[float], dp: list[float]) -> float:\n",
    "        \"\"\"Calculate the lifted index using metpy\"\"\"\n",
    "\n",
    "        # Conver the units\n",
    "        pres = np.array(pres) * units.hPa\n",
    "        temp = np.array(temp) * units.degC\n",
    "        dp = np.array(dp) * units.degC\n",
    "\n",
    "        # Calculate parameters\n",
    "        try:\n",
    "            # Calculate the parcel profile\n",
    "            prof = parcel_profile(pres, temp[0], dp[0])\n",
    "\n",
    "            # The calculation throws an exception if the data doesn't reach the EL\n",
    "            li = lifted_index(pres, temp, prof)\n",
    "\n",
    "            return float(li.magnitude[0])\n",
    "        except:\n",
    "            return math.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing USM00072249-data.txt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-2fa3a1d50bf2>:148: UserWarning: Interpolation point out of data bounds encountered\n",
      "  li = lifted_index(pres, temp, prof)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Read 49486 headers, 4387044 lines. Filtered 31519. Errors 30. Bad data 627. Wrote 17310 records.\n",
      "Processing USM00072250-data.txt.\n"
     ]
    }
   ],
   "source": [
    "# Set up for processing\n",
    "callbacks = LiftedIndex(SILVER_LI_PATH, datetime(2000, 1, 1))\n",
    "reader = olieigra.Reader(callbacks=callbacks)\n",
    "crawler = olieigra.Crawler(reader=reader)\n",
    "\n",
    "# Crawl and process files\n",
    "crawler.crawl(BRONZE_DATA_POR_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olieigra_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
