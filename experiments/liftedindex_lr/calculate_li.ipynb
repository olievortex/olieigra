{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate LI\n",
    "This notebook calcualtes the lifted index values for us to train models with. See the README to learn what the Lifted Index is. It reads in the data we generated from the transform_data_gph20s10k notebook. It then unpivots the data, converting columns back into pressure levels. This data can then be used by MetPy to calculate the Lifted Index using the actual physics algorithms. A CSV file is geratated for each input file.\n",
    "\n",
    "Update the following parameters in the first cell to accomodate your installation:\n",
    "\n",
    "- SILVER_GPH20S10K_PATH - The location to read the transformed CSV files\n",
    "- SILVER_LI_PATH - Location to save the calculated Lifted Index values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "SILVER_GPH20S10K_PATH = '/lakehouse/default/Files/silver/igra2/gph20s10k'\n",
    "SILVER_LI_PATH = '/lakehouse/default/Files/silver/igra2/li'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the destination folder exists\n",
    "os.makedirs(SILVER_LI_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "class LiftedIndex:\n",
    "    feature_cols = 4\n",
    "\n",
    "    def __init__(self, dst_path: str):\n",
    "        self.dst_path = dst_path\n",
    "        self.pivot_columns = ['gph','pres','temp','dp','u','v']\n",
    "\n",
    "    def crawl(self, path: str):\n",
    "        \"\"\"Scan a folder for data files and process them\"\"\"\n",
    "\n",
    "        # Iterate through each file in the path\n",
    "        for file in os.listdir(path):\n",
    "            # Skip if the filename doesn't match the pattern we're expect\n",
    "            if not file.endswith('-data-gph20s10k.csv'):\n",
    "                print(f'Skipping {file}. Not sure what to do with it.')\n",
    "                continue\n",
    "\n",
    "            # Figure out the filenames\n",
    "            src_filename = f'{path}/{file}'\n",
    "            dst_filename = f\"{self.dst_path}/{file.replace('gph20s10k.csv', 'li.csv')}\"\n",
    "            tmp_filename = dst_filename.replace('.csv', '.partial.csv')\n",
    "           \n",
    "            # Skip the file if it has already been processed\n",
    "            if os.path.exists(dst_filename):\n",
    "                print(f'Skipping {file}. Destination file already exists.')\n",
    "                continue\n",
    "\n",
    "            # Process the file\n",
    "            print(f'Processing {file}.')\n",
    "            self.process(src_filename, tmp_filename)\n",
    "\n",
    "    def process(self, src_filename: str, tmp_filename: str):\n",
    "        \"\"\"Process a data file\"\"\"\n",
    "\n",
    "        # Open the reader and writer streams\n",
    "        with open(src_filename, 'r') as reader, open(tmp_filename, 'w') as writer:\n",
    "            # Figure out how many levels we are dealing with\n",
    "            num_levels = self.read_header(reader)\n",
    "\n",
    "            # Loop through the data rows\n",
    "            self.process_loop(reader, num_levels)\n",
    "\n",
    "    def process_loop(self, reader: io.TextIOWrapper, num_levels: int):\n",
    "        \"\"\"Process all the data rows in a file\"\"\"\n",
    "\n",
    "        # Loop through the data rows\n",
    "        while True:\n",
    "            # Read the next record\n",
    "            line = reader.readline()\n",
    "\n",
    "            # Exit the loop if EOF\n",
    "            if line == \"\":\n",
    "                break;\n",
    "\n",
    "            # Unpivot the row\n",
    "            data = self.read_row(line, num_levels)\n",
    "\n",
    "    def read_header(self, reader: io.TextIOWrapper) -> int:\n",
    "        \"\"\"Process the file's header row\"\"\"\n",
    "\n",
    "        # Read the first line\n",
    "        line = reader.readline()\n",
    "\n",
    "        # Skip over the dimension columns and get the feature columns\n",
    "        parts = np.array(line.split(',')[self.feature_cols:])\n",
    "        num_levels = int(len(parts) / len(self.pivot_columns))\n",
    "        \n",
    "        return num_levels\n",
    "\n",
    "    def read_row(self, line: str, num_levels: int) -> pd.DataFrame:\n",
    "        \"\"\"Read a line and depivot the data\"\"\"\n",
    "        gph = []        \n",
    "        p = []\n",
    "        t = []\n",
    "        dp = []\n",
    "        u = []\n",
    "        v = []\n",
    "\n",
    "        parts = line.split(',')\n",
    "\n",
    "        for level in range(num_levels):\n",
    "            ptr = level * 6 + self.feature_cols\n",
    "            gph.append(float(parts[ptr]))\n",
    "            p.append(float(parts[ptr + 1]))\n",
    "            t.append(float(parts[ptr + 2]))\n",
    "            dp.append(float(parts[ptr + 3]))\n",
    "            u.append(float(parts[ptr + 4]))\n",
    "            v.append(float(parts[ptr + 5]))\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "            'gph': gph,\n",
    "            'p': p,\n",
    "            't': t,\n",
    "            'dp': dp,\n",
    "            'u': u,\n",
    "            'v': v})\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # def calc_li_from_csv(self, in_filename: str, out_filename: str):\n",
    "    #     \"\"\"Make the LI prediction dataset\"\"\"\n",
    "    #     ds_csv = DatasourceIgraCsv()\n",
    "    #     start_time = datetime.now()\n",
    "    #     iter_num = 0\n",
    "    #     error_nan = 0\n",
    "    #     good = 0\n",
    "\n",
    "    #     with open(in_filename, 'r', encoding='UTF-8') as sr, open(out_filename, 'w', encoding='UTF-8') as sw:\n",
    "    #         header = ds_csv.read_header(sr.readline())\n",
    "\n",
    "    #         sw.write('lifted_index\\n')\n",
    "\n",
    "    #         while True:\n",
    "    #             # region Check for EOF\n",
    "\n",
    "    #             line = sr.readline()\n",
    "    #             if line == \"\":\n",
    "    #                 break\n",
    "\n",
    "    #             # endregion\n",
    "\n",
    "    #             # region Progress Indication\n",
    "\n",
    "    #             if iter_num % 25 == 0:\n",
    "    #                 print(\n",
    "    #                     f\"\\rRow: {iter_num} {(datetime.now()-start_time).seconds}s\", end=\"\")\n",
    "    #             iter_num += 1\n",
    "\n",
    "    #             # endregion\n",
    "\n",
    "    #             row = ds_csv.read_row(line, header['num_levels'])\n",
    "\n",
    "    #             pres = np.array(row['p']) * units.Pa\n",
    "    #             temp = np.array(row['t']) / 10.0 * units.degC\n",
    "    #             dp = np.array(row['dp']) / 10.0 * units.degC\n",
    "\n",
    "    #             if np.isnan(temp).any() or np.isnan(dp).any():\n",
    "    #                 error_nan += 1\n",
    "    #                 sw.write(\"nan\\n\")\n",
    "    #                 continue\n",
    "\n",
    "    #             # compute the parcel temperatures from surface parcel\n",
    "    #             prof = parcel_profile(pres, temp[0], dp[0])\n",
    "\n",
    "    #             # calculate the LI\n",
    "    #             li = float(lifted_index(pres, temp, prof).magnitude[0])\n",
    "    #             good += 1\n",
    "\n",
    "    #             sw.write(f\"{li}\\n\")\n",
    "\n",
    "    #     print(\n",
    "    #         f\"\\rTotal: {iter_num}, Good: {good}, NaNs: {error_nan}, Time taken: {(datetime.now()-start_time).seconds}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing USM00072249-data-gph20s10k.csv.\n",
      "Processing USM00072250-data-gph20s10k.csv.\n",
      "Processing USM00072251-data-gph20s10k.csv.\n",
      "Processing USM00072261-data-gph20s10k.csv.\n",
      "Processing USM00072265-data-gph20s10k.csv.\n",
      "Processing USM00072357-data-gph20s10k.csv.\n",
      "Processing USM00072363-data-gph20s10k.csv.\n",
      "Processing USM00072364-data-gph20s10k.csv.\n",
      "Processing USM00072440-data-gph20s10k.csv.\n",
      "Processing USM00072451-data-gph20s10k.csv.\n",
      "Processing USM00072456-data-gph20s10k.csv.\n",
      "Processing USM00072476-data-gph20s10k.csv.\n",
      "Processing USM00072558-data-gph20s10k.csv.\n",
      "Processing USM00072562-data-gph20s10k.csv.\n",
      "Processing USM00072645-data-gph20s10k.csv.\n",
      "Processing USM00072649-data-gph20s10k.csv.\n",
      "Processing USM00072659-data-gph20s10k.csv.\n",
      "Processing USM00072662-data-gph20s10k.csv.\n",
      "Processing USM00072747-data-gph20s10k.csv.\n",
      "Processing USM00072764-data-gph20s10k.csv.\n",
      "Processing USM00074455-data-gph20s10k.csv.\n",
      "Processing USM00074560-data-gph20s10k.csv.\n",
      "Processing USM00074646-data-gph20s10k.csv.\n"
     ]
    }
   ],
   "source": [
    "li = LiftedIndex(SILVER_LI_PATH)\n",
    "li.crawl(SILVER_GPH20S10K_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olieigra_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
