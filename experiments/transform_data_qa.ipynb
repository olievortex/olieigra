{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Data for Analysis\n",
    "This notebook aggregates data to help analyze the igra data. The main goal is to figure out the granularity of the data in terms of geopotential height. In particular, I am interested in the number of observations in the troposphere, where most of the weather happens. I arbitrarily decided the \"top\" of the troposphere is 10km.\n",
    "\n",
    "For each observation, we filter out:\n",
    "- Records older than 1/1/2000\n",
    "- Non-pressure records\n",
    "- Records where an observation is missing or invalid\n",
    "- Observations where there isn't a valid surface record\n",
    "\n",
    "For each observation, we find:\n",
    "- Max Geopotential Height\n",
    "- Minimum Pressure\n",
    "- Total number of valid records in the observation\n",
    "- Number of valid records below 10km in geopotential height\n",
    "\n",
    "Update the following parameters in the first cell to accomodate your installation:\n",
    "\n",
    "- BRONZE_DATA_POR_PATH - The location of the raw data-por zip files\n",
    "- SILVER_QA_PATH - The location to save the transformed CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need to install olieigra, uncomment and execute this line. View the README in the project root for instructions\n",
    "# on how to build or download this file.\n",
    "#%pip install /lakehouse/default/Files/libs/olieigra-0.0.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from datetime import datetime\n",
    "import olieigra\n",
    "\n",
    "BRONZE_DATA_POR_PATH = '/Users/olievortex/lakehouse/default/Files/bronze/igra2/data-por'\n",
    "SILVER_QA_PATH = '/Users/olievortex/lakehouse/default/Files/silver/igra2/qa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the destination path exists\n",
    "os.makedirs(SILVER_QA_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The business logic is contained within this class\n",
    "\n",
    "class QualityAnalysis(olieigra.Callbacks):\n",
    "    def __init__(self, dst_path: str, min_effective_date: datetime):\n",
    "        super().__init__()\n",
    "        self.min_effective_date = min_effective_date\n",
    "        self.dst_path = dst_path\n",
    "        self.filename = ''\n",
    "        self.filtered = 0\n",
    "        self.writer = None\n",
    "        self.hout = ''\n",
    "\n",
    "    def start_file(self, filename: str) -> bool:\n",
    "        \"\"\"Decide if we want to process the file. If so, reset state and start writing to a\n",
    "        temporary file.\"\"\"\n",
    "\n",
    "        # An IGRA2 file should end with -data.txt\n",
    "        if not filename.endswith('-data.txt'):\n",
    "            print(f'Skipping {filename}. Not sure what to do with it.')\n",
    "            return False\n",
    "\n",
    "        # Set the desired destination filename\n",
    "        dst_filename = f'{self.dst_path}/{filename}'\n",
    "        dst_filename = dst_filename.replace(\"-data.txt\", \"-data-qa.csv\")\n",
    "\n",
    "        # Skip this file if it has already been processed\n",
    "        if os.path.exists(dst_filename):\n",
    "            print(f'Skipping {filename}. Destination file already exists.')\n",
    "            return False\n",
    "\n",
    "        # If we got here, we are going to process the file\n",
    "        print(f'Processing {filename}.')\n",
    "\n",
    "        # Write to a temp file\n",
    "        self.filename = dst_filename.replace('-data-qa.csv', '-data-qa.partial.csv')\n",
    "        self.writer = open(self.filename, 'w', encoding='UTF-8')\n",
    "\n",
    "        # Reset the filtered record count\n",
    "        self.filtered = 0\n",
    "        self.hout = ''\n",
    "\n",
    "        # Write the header row\n",
    "        self.writer.write('id,effective_date,hour,usable_10k,usable_all,max_gph,min_pa\\n')\n",
    "\n",
    "        # Tell olieigra to continue processing\n",
    "        return True\n",
    "    \n",
    "    def finish_file(self, headers: int, rows: int):\n",
    "        \"\"\"File processing is complete. Clean up and provide user feedback.\"\"\"\n",
    "\n",
    "        # Close the temporary file\n",
    "        self.writer.close()\n",
    "\n",
    "        # Rename the temporary file\n",
    "        dst_renamed = self.filename.replace('.partial.csv', '.csv')\n",
    "        os.rename(self.filename, dst_renamed)\n",
    "\n",
    "        # Calculate the number of records written\n",
    "        loaded = headers - self.filtered\n",
    "\n",
    "        # Provide feedback to the user\n",
    "        print(f\" Read {headers} headers, {rows} lines. Filtered {self.filtered}. \" +\n",
    "              f\"Wrote {loaded} records.\")\n",
    "\n",
    "    def parse_header(self, header: olieigra.HeaderModel) -> bool:\n",
    "        \"\"\"Transform the header record and start writing a record\"\"\"\n",
    "\n",
    "        # Combine the separate fields into a date\n",
    "        effective_date = datetime(header.year, header.month, header.day)\n",
    "\n",
    "        # Skip the record if it is too old\n",
    "        if effective_date < self.min_effective_date:\n",
    "            self.filtered += 1\n",
    "            return False\n",
    "\n",
    "        # We may not write the row, so save the header columns in a variable for now\n",
    "        self.hout = f'{header.id},{effective_date:%Y-%m-%d},{header.hour}'\n",
    "\n",
    "        # Tell olieigra to process the body associated with this header\n",
    "        return True\n",
    "    \n",
    "    def parse_body(self, body: list[olieigra.BodyModel]):\n",
    "        \"\"\"Perform some analytics and finish writing a record\"\"\"\n",
    "\n",
    "        # Initialize\n",
    "        usable_10k = 0\n",
    "        usable_all = 0\n",
    "        surface_nan = 1\n",
    "        max_gph = -1\n",
    "        min_pa = 99999999\n",
    "\n",
    "        # Iterate through each record in the body\n",
    "        for item in body:\n",
    "            # We don't care about non-pressure records\n",
    "            if item.type[0] == '3':\n",
    "                continue\n",
    "\n",
    "            # We don't care about records that contain a NaN value\n",
    "            if math.isnan(item.dpdp) | math.isnan(item.rh) | math.isnan(item.temp) | \\\n",
    "                    math.isnan(item.wdir) | math.isnan(item.wspd) | math.isnan(item.gph):\n",
    "                continue\n",
    "\n",
    "            # Find the highest gph\n",
    "            if item.gph > max_gph:\n",
    "                max_gph = item.gph\n",
    "\n",
    "            # Find the lowest pa\n",
    "            if item.pres < min_pa:\n",
    "                min_pa = item.pres\n",
    "\n",
    "            # This is a usable record\n",
    "            usable_all += 1\n",
    "\n",
    "            # We don't care about records higher than 10km\n",
    "            if item.gph > 10000:\n",
    "                continue\n",
    "\n",
    "            # Flag that we found a valid surface record\n",
    "            if item.type == '21':\n",
    "                surface_nan = 0\n",
    "\n",
    "            # If we got here, the record is usable and lower then 10k height\n",
    "            usable_10k += 1\n",
    "\n",
    "        # Write record if there was a valid surface record\n",
    "        if surface_nan == 0:\n",
    "            self.writer.write(f'{self.hout},{usable_10k},{usable_all},{max_gph},{min_pa}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing USM00072251-data.txt.\n",
      " Read 34081 headers, 4532303 lines. Filtered 16310. Wrote 17771 records.\n",
      "Processing USM00072357-data.txt.\n",
      " Read 26409 headers, 4084416 lines. Filtered 8706. Wrote 17703 records.\n",
      "Processing USM00072250-data.txt.\n",
      " Read 77417 headers, 5455633 lines. Filtered 59498. Wrote 17919 records.\n",
      "Processing USM00072456-data.txt.\n",
      " Read 54326 headers, 5136424 lines. Filtered 36402. Wrote 17924 records.\n",
      "Processing USM00072451-data.txt.\n",
      " Read 65711 headers, 4849890 lines. Filtered 47852. Wrote 17859 records.\n",
      "Processing USM00072649-data.txt.\n",
      " Read 26612 headers, 3659546 lines. Filtered 9224. Wrote 17388 records.\n",
      "Processing USM00072261-data.txt.\n",
      " Read 53682 headers, 5133602 lines. Filtered 35986. Wrote 17696 records.\n",
      "Processing USM00074560-data.txt.\n",
      " Read 21214 headers, 3761759 lines. Filtered 3539. Wrote 17675 records.\n",
      "Processing USM00072645-data.txt.\n",
      " Read 61708 headers, 4823579 lines. Filtered 44399. Wrote 17309 records.\n",
      "Processing USM00072249-data.txt.\n",
      " Read 49737 headers, 4446304 lines. Filtered 31519. Wrote 18218 records.\n",
      "Processing USM00072747-data.txt.\n",
      " Read 61276 headers, 4818934 lines. Filtered 44039. Wrote 17237 records.\n",
      "Processing USM00072662-data.txt.\n",
      " Read 69285 headers, 4949468 lines. Filtered 51686. Wrote 17599 records.\n",
      "Processing USM00072558-data.txt.\n",
      " Read 83354 headers, 5105789 lines. Filtered 65841. Wrote 17513 records.\n",
      "Processing USM00074455-data.txt.\n",
      " Read 23193 headers, 3857507 lines. Filtered 5560. Wrote 17633 records.\n",
      "Processing USM00072764-data.txt.\n",
      " Read 78952 headers, 5027576 lines. Filtered 61574. Wrote 17378 records.\n",
      "Processing USM00072659-data.txt.\n",
      " Read 21198 headers, 3718703 lines. Filtered 3693. Wrote 17505 records.\n",
      "Processing USM00074646-data.txt.\n",
      " Read 25608 headers, 2377189 lines. Filtered 0. Wrote 25608 records.\n",
      "Processing USM00072363-data.txt.\n",
      " Read 76459 headers, 4914828 lines. Filtered 59076. Wrote 17383 records.\n",
      "Processing USM00072364-data.txt.\n",
      " Read 75625 headers, 5000396 lines. Filtered 58112. Wrote 17513 records.\n",
      "Processing USM00072440-data.txt.\n",
      " Read 32343 headers, 3866497 lines. Filtered 14599. Wrote 17744 records.\n",
      "Processing USM00072562-data.txt.\n",
      " Read 76207 headers, 5081951 lines. Filtered 58738. Wrote 17469 records.\n",
      "Processing USM00072265-data.txt.\n",
      " Read 62424 headers, 4934369 lines. Filtered 44950. Wrote 17474 records.\n",
      "Processing USM00072476-data.txt.\n",
      " Read 68763 headers, 4779304 lines. Filtered 51675. Wrote 17088 records.\n"
     ]
    }
   ],
   "source": [
    "# Set up for processing\n",
    "callbacks = QualityAnalysis(SILVER_QA_PATH, datetime(2000, 1, 1))\n",
    "reader = olieigra.Reader(callbacks=callbacks)\n",
    "crawler = olieigra.Crawler(reader=reader)\n",
    "\n",
    "# Crawl and process files\n",
    "crawler.crawl(BRONZE_DATA_POR_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olieigra_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
