{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Data for Analysis\n",
    "This notebook aggregates data to help analyze the igra data. The main goal is to figure out the granularity of the data in terms of geopotential height. In particular, I am interested in the number of observations in the troposphere, where most of the weather happens. I arbitrarily decided the \"top\" of the troposphere is 10km.\n",
    "\n",
    "For each observation, we filter out:\n",
    "- Records older than 1/1/2000\n",
    "- Non-pressure records\n",
    "- Records where an observation is missing or invalid\n",
    "- Observations where there isn't a valid surface record\n",
    "\n",
    "For each observation, we find:\n",
    "- Max Geopotential Height\n",
    "- Minimum Pressure\n",
    "- Total number of valid records in the observation\n",
    "- Number of valid records below 10km in geopotential height\n",
    "\n",
    "Update the following parameters in the first cell to accomodate your installation:\n",
    "\n",
    "- BRONZE_DATA_POR_PATH - The location of the raw data-por zip files\n",
    "- SILVER_QA_PATH - The location to save the transformed CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need to install olieigra, uncomment and execute this line. View the README in the project root for instructions\n",
    "# on how to build or download this file.\n",
    "#%pip install /lakehouse/default/Files/libs/olieigra-0.0.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from datetime import datetime\n",
    "import olieigra\n",
    "\n",
    "BRONZE_DATA_POR_PATH = '/lakehouse/default/Files/bronze/igra2/data-por'\n",
    "SILVER_QA_PATH = '/lakehouse/default/Files/silver/igra2/qa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the destination path exists\n",
    "os.makedirs(SILVER_QA_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The business logic is contained within this class\n",
    "\n",
    "class QualityAnalysis(olieigra.Callbacks):\n",
    "    def __init__(self, dst_path: str, min_effective_date: datetime):\n",
    "        super().__init__()\n",
    "        self.min_effective_date = min_effective_date\n",
    "        self.dst_path = dst_path\n",
    "        self.filename = ''\n",
    "        self.filtered = 0\n",
    "        self.writer = None\n",
    "        self.hout = ''\n",
    "\n",
    "    def start_file(self, filename: str) -> bool:\n",
    "        \"\"\"Decide if we want to process the file. If so, reset state and start writing to a\n",
    "        temporary file.\"\"\"\n",
    "\n",
    "        # An IGRA2 file should end with -data.txt\n",
    "        if not filename.endswith('-data.txt'):\n",
    "            print(f'Skipping {filename}. Not sure what to do with it.')\n",
    "            return False\n",
    "\n",
    "        # Set the desired destination filename\n",
    "        dst_filename = f'{self.dst_path}/{filename}'\n",
    "        dst_filename = dst_filename.replace(\"-data.txt\", \"-data-qa.csv\")\n",
    "\n",
    "        # Skip this file if it has already been processed\n",
    "        if os.path.exists(dst_filename):\n",
    "            print(f'Skipping {filename}. Destination file already exists.')\n",
    "            return False\n",
    "\n",
    "        # If we got here, we are going to process the file\n",
    "        print(f'Processing {filename}.')\n",
    "\n",
    "        # Write to a temp file\n",
    "        self.filename = dst_filename.replace('-data-qa.csv', '-data-qa.partial.csv')\n",
    "        self.writer = open(self.filename, 'w', encoding='UTF-8')\n",
    "\n",
    "        # Reset the filtered record count\n",
    "        self.filtered = 0\n",
    "        self.hout = ''\n",
    "\n",
    "        # Write the header row\n",
    "        self.writer.write('id,effective_date,hour,usable_10k,usable_all,max_gph,min_pa\\n')\n",
    "\n",
    "        # Tell olieigra to continue processing\n",
    "        return True\n",
    "    \n",
    "    def finish_file(self, headers: int, rows: int):\n",
    "        \"\"\"File processing is complete. Clean up and provide user feedback.\"\"\"\n",
    "\n",
    "        # Close the temporary file\n",
    "        self.writer.close()\n",
    "\n",
    "        # Rename the temporary file\n",
    "        dst_renamed = self.filename.replace('.partial.csv', '.csv')\n",
    "        os.rename(self.filename, dst_renamed)\n",
    "\n",
    "        # Calculate the number of records written\n",
    "        loaded = headers - self.filtered\n",
    "\n",
    "        # Provide feedback to the user\n",
    "        print(f\" Read {headers} headers, {rows} lines. Filtered {self.filtered}. \" +\n",
    "              f\"Wrote {loaded} records.\")\n",
    "\n",
    "    def parse_header(self, header: olieigra.HeaderModel) -> bool:\n",
    "        \"\"\"Transform the header record and start writing a record\"\"\"\n",
    "\n",
    "        # Combine the separate fields into a date\n",
    "        effective_date = datetime(header.year, header.month, header.day)\n",
    "\n",
    "        # Skip the record if it is too old\n",
    "        if effective_date < self.min_effective_date:\n",
    "            self.filtered += 1\n",
    "            return False\n",
    "\n",
    "        # We may not write the row, so save the header columns in a variable for now\n",
    "        self.hout = f'{header.id},{effective_date:%Y-%m-%d},{header.hour}'\n",
    "\n",
    "        # Tell olieigra to process the body associated with this header\n",
    "        return True\n",
    "    \n",
    "    def parse_body(self, body: list[olieigra.BodyModel]):\n",
    "        \"\"\"Perform some analytics and finish writing a record\"\"\"\n",
    "\n",
    "        # Initialize\n",
    "        usable_10k = 0\n",
    "        usable_all = 0\n",
    "        surface_nan = 1\n",
    "        max_gph = -1\n",
    "        min_pa = 99999999\n",
    "\n",
    "        # Iterate through each record in the body\n",
    "        for item in body:\n",
    "            # We don't care about non-pressure records\n",
    "            if item.type[0] == '3':\n",
    "                continue\n",
    "\n",
    "            # We don't care about records that contain a NaN value\n",
    "            if math.isnan(item.dpdp) | math.isnan(item.rh) | math.isnan(item.temp) | \\\n",
    "                    math.isnan(item.wdir) | math.isnan(item.wspd) | math.isnan(item.gph):\n",
    "                continue\n",
    "\n",
    "            # Find the highest gph\n",
    "            if item.gph > max_gph:\n",
    "                max_gph = item.gph\n",
    "\n",
    "            # Find the lowest pa\n",
    "            if item.pres < min_pa:\n",
    "                min_pa = item.pres\n",
    "\n",
    "            # This is a usable record\n",
    "            usable_all += 1\n",
    "\n",
    "            # We don't care about records higher than 10km\n",
    "            if item.gph > 10000:\n",
    "                continue\n",
    "\n",
    "            # Flag that we found a valid surface record\n",
    "            if item.type == '21':\n",
    "                surface_nan = 0\n",
    "\n",
    "            # If we got here, the record is usable and lower then 10k height\n",
    "            usable_10k += 1\n",
    "\n",
    "        # Write record if there was a valid surface record\n",
    "        if surface_nan == 0:\n",
    "            self.writer.write(f'{self.hout},{usable_10k},{usable_all},{max_gph},{min_pa}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing USM00072249-data.txt.\n",
      " Read 49486 headers, 4387044 lines. Filtered 31519. Wrote 17967 records.\n",
      "Processing USM00072250-data.txt.\n",
      " Read 77165 headers, 5392499 lines. Filtered 59498. Wrote 17667 records.\n",
      "Processing USM00072251-data.txt.\n",
      " Read 33828 headers, 4472651 lines. Filtered 16310. Wrote 17518 records.\n",
      "Processing USM00072261-data.txt.\n",
      " Read 53638 headers, 5125305 lines. Filtered 35986. Wrote 17652 records.\n",
      "Processing USM00072265-data.txt.\n",
      " Read 62172 headers, 4864626 lines. Filtered 44950. Wrote 17222 records.\n",
      "Processing USM00072357-data.txt.\n",
      " Read 26154 headers, 4024085 lines. Filtered 8706. Wrote 17448 records.\n",
      "Processing USM00072363-data.txt.\n",
      " Read 76207 headers, 4841009 lines. Filtered 59076. Wrote 17131 records.\n",
      "Processing USM00072364-data.txt.\n",
      " Read 75378 headers, 4930217 lines. Filtered 58112. Wrote 17266 records.\n",
      "Processing USM00072440-data.txt.\n",
      " Read 32092 headers, 3776344 lines. Filtered 14599. Wrote 17493 records.\n",
      "Processing USM00072451-data.txt.\n",
      " Read 65460 headers, 4793099 lines. Filtered 47852. Wrote 17608 records.\n",
      "Processing USM00072456-data.txt.\n",
      " Read 54072 headers, 5076564 lines. Filtered 36402. Wrote 17670 records.\n",
      "Processing USM00072476-data.txt.\n",
      " Read 68511 headers, 4701461 lines. Filtered 51675. Wrote 16836 records.\n",
      "Processing USM00072558-data.txt.\n",
      " Read 83102 headers, 5030812 lines. Filtered 65841. Wrote 17261 records.\n",
      "Processing USM00072562-data.txt.\n",
      " Read 75955 headers, 5005187 lines. Filtered 58738. Wrote 17217 records.\n",
      "Processing USM00072645-data.txt.\n",
      " Read 61456 headers, 4748359 lines. Filtered 44399. Wrote 17057 records.\n",
      "Processing USM00072649-data.txt.\n",
      " Read 26360 headers, 3578840 lines. Filtered 9224. Wrote 17136 records.\n",
      "Processing USM00072659-data.txt.\n",
      " Read 20946 headers, 3644897 lines. Filtered 3693. Wrote 17253 records.\n",
      "Processing USM00072662-data.txt.\n",
      " Read 69033 headers, 4872393 lines. Filtered 51686. Wrote 17347 records.\n",
      "Processing USM00072747-data.txt.\n",
      " Read 61025 headers, 4742905 lines. Filtered 44039. Wrote 16986 records.\n",
      "Processing USM00072764-data.txt.\n",
      " Read 78702 headers, 4947724 lines. Filtered 61574. Wrote 17128 records.\n",
      "Processing USM00074455-data.txt.\n",
      " Read 22942 headers, 3788681 lines. Filtered 5560. Wrote 17382 records.\n",
      "Processing USM00074560-data.txt.\n",
      " Read 20962 headers, 3686671 lines. Filtered 3539. Wrote 17423 records.\n",
      "Processing USM00074646-data.txt.\n",
      " Read 25362 headers, 2344781 lines. Filtered 0. Wrote 25362 records.\n"
     ]
    }
   ],
   "source": [
    "# Set up for processing\n",
    "callbacks = QualityAnalysis(SILVER_QA_PATH, datetime(2000, 1, 1))\n",
    "reader = olieigra.Reader(callbacks=callbacks)\n",
    "crawler = olieigra.Crawler(reader=reader)\n",
    "\n",
    "# Crawl and process files\n",
    "crawler.crawl(BRONZE_DATA_POR_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olieigra_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
