{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Data gph20s10k\n",
    "Transform the data-por observations into a format we can use for machine learning. The problem is the samples in an observation are not consistent between observations. The solution is to interpolate data into standardized levels. We split the samples into 20 levels between the surface and 10km. This allows all the observations to be consistent for a given station. Although the number of levels are the same between other stations, because the surface elevation is different, the actual altitudes of each slice will be different.\n",
    "\n",
    "Observations are also filtered on quality. If there are less than 20 samples without data problems between the surface and 10km geopotential height, the entire observation is rejected. If a surface sample has any invalid data, the entire observation is also rejected.\n",
    "\n",
    "Update the following parameters in the first cell to accomodate your installation:\n",
    "\n",
    "- BRONZE_DATA_POR_PATH - The location of the raw data-por zip files\n",
    "- SILVER_GPH20S10K_PATH - The location to save the transformed CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need to install olieigra, uncomment and execute this line. View the README in the project root for instructions\n",
    "# on how to build or download this file.\n",
    "#%pip install /lakehouse/default/Files/libs/olieigra-0.0.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import olieigra\n",
    "\n",
    "BRONZE_DATA_POR_PATH = '/lakehouse/default/Files/bronze/igra2/data-por'\n",
    "SILVER_GPH20S10K_PATH = '/lakehouse/default/Files/silver/igra2/gph20s10k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the destination path exists\n",
    "os.makedirs(SILVER_GPH20S10K_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The business logic is contained within this class\n",
    "\n",
    "class Gph20S10K(olieigra.Callbacks):\n",
    "    samples = 21\n",
    "    gph_top = 10000\n",
    "    min_usable = 20\n",
    "\n",
    "    def __init__(self, dst_path: str, min_effective_date: datetime):\n",
    "        super().__init__()\n",
    "        self.dst_path = dst_path\n",
    "        self.filename = ''\n",
    "        self.filtered = 0\n",
    "        self.rejected = 0\n",
    "        self.writer = None\n",
    "        self.min_effective_date = min_effective_date\n",
    "        self.hout = ''\n",
    "        self.levels = []\n",
    "\n",
    "    def start_file(self, filename: str) -> bool:\n",
    "        \"\"\"Decide if we want to process the file. If so, reset state and start writing to a\n",
    "        temporary file.\"\"\"\n",
    "\n",
    "        # An IGRA2 file should end with -data.txt\n",
    "        if not filename.endswith('-data.txt'):\n",
    "            print(f'Skipping {filename}. Not sure what to do with it.')\n",
    "            return False\n",
    "\n",
    "        # Set the desired destination filename\n",
    "        dst_filename = f'{self.dst_path}/{filename}'\n",
    "        dst_filename = dst_filename.replace(\"-data.txt\", \"-data-gph20s10k.csv\")\n",
    "\n",
    "        # Skip this file if it has already been processed\n",
    "        if os.path.exists(dst_filename):\n",
    "            print(f'Skipping {filename}. Destination file already exists.')\n",
    "            return False\n",
    "\n",
    "        # If we got here, we are going to process the file\n",
    "        print(f'Processing {filename}.')\n",
    "\n",
    "        # Write to a temp file\n",
    "        self.filename = dst_filename.replace('.csv', '.partial.csv')\n",
    "        self.writer = open(self.filename, 'w', encoding='UTF-8')\n",
    "        self.hout = ''\n",
    "\n",
    "        # Reset the record counts\n",
    "        self.filtered = 0\n",
    "        self.rejected = 0\n",
    "\n",
    "        # Write the header row\n",
    "        attr = ['gph','pres','temp','dp','u','v']\n",
    "        dynamic = ','.join([f'{level}_{x}'\n",
    "                            for level in range(self.samples)\n",
    "                            for x in attr])\n",
    "        self.writer.write(f\"id,effective_date,hour,day_num,{dynamic}\\n\")\n",
    "\n",
    "        # Tell olieigra to continue processing\n",
    "        return True\n",
    "\n",
    "    def finish_file(self, headers: int, rows: int):\n",
    "        \"\"\"Callback for when processing is complete\"\"\"\n",
    "\n",
    "        # Flush and close the temp file\n",
    "        self.writer.close()\n",
    "\n",
    "        # Rename it to the final filename\n",
    "        dst_renamed = self.filename.replace('.partial.csv', '.csv')\n",
    "        os.rename(self.filename, dst_renamed)\n",
    "\n",
    "        # Calculate the number of records written\n",
    "        loaded = headers - self.filtered - self.rejected\n",
    "\n",
    "        print(f\" Read {headers} headers, {rows} lines. Filtered {self.filtered}. \" +\n",
    "              f\"Rejected {self.rejected}. Wrote {loaded} records.\")\n",
    "\n",
    "    def parse_header(self, header: olieigra.HeaderModel):\n",
    "        \"\"\"Transform the header record\"\"\"\n",
    "\n",
    "        # Combine seperate fields into a datetime\n",
    "        effective_date = datetime(header.year, header.month, header.day)\n",
    "\n",
    "        # Filter out the observations that are too old\n",
    "        if effective_date < self.min_effective_date:\n",
    "            self.filtered += 1\n",
    "            return False\n",
    "\n",
    "        # We need some number that is analogous to the amount of sunlight and the season\n",
    "        day_num = -math.cos(math.radians(effective_date.timetuple().tm_yday))\n",
    "\n",
    "        # The observation may be rejected due to body data issues. Save the header values to\n",
    "        # a variable for now. The parse_body will write it to the file, if appropriate.\n",
    "        self.hout = f'{header.id},{effective_date:%Y-%m-%d},{header.hour},{day_num:.2f}'\n",
    "\n",
    "        # Continue the processing\n",
    "        return True\n",
    "\n",
    "    def parse_body(self, body: list[olieigra.BodyModel]):\n",
    "        \"\"\"Perform some analytics on the body\"\"\"\n",
    "\n",
    "        # Remove non-pressure records and records with bad data\n",
    "        filtered = self.filter_body(body)\n",
    "\n",
    "        # If the obs failed validation checks, skip it\n",
    "        if len(filtered) == 0:\n",
    "            self.rejected += 1\n",
    "            return\n",
    "\n",
    "        # Have we calculated the levels yet?\n",
    "        if len(self.levels) == 0:\n",
    "            # We can't calculate the levels until we've had a surface sample\n",
    "            self.levels = np.linspace(filtered[0][0], self.gph_top, self.samples)\n",
    "\n",
    "        # Convert rows to columns and interpolate to our standard levels\n",
    "        pivoted = self.body_pivot(filtered)\n",
    "\n",
    "        # Combine the results to a comma delimited list\n",
    "        out = ','.join([f\"{item:.1f}\" for item in pivoted])\n",
    "\n",
    "        # Write the record\n",
    "        self.writer.write(f'{self.hout},{out}\\n')\n",
    "\n",
    "    def filter_body(self, body: list[olieigra.BodyModel]) -> list[list[float]]:\n",
    "        \"\"\"Filter out bad data\"\"\"\n",
    "        result = [[], [], [], [], [], []]\n",
    "        usable_count = 0\n",
    "        surface_nan = 1\n",
    "        last_gph = -1\n",
    "\n",
    "        # Iterate over every body record\n",
    "        for item in body:\n",
    "            # If we have at least one record over 10k in height, we have enough data to interpolate\n",
    "            if last_gph >= 10000:\n",
    "                break\n",
    "\n",
    "            # Skip non-pressure records\n",
    "            if item.type[0] == '3':\n",
    "                continue\n",
    "\n",
    "            # Skip records with bad or missing data\n",
    "            if math.isnan(item.dpdp) | math.isnan(item.rh) | math.isnan(item.temp) | \\\n",
    "                    math.isnan(item.wdir) | math.isnan(item.wspd) | math.isnan(item.gph):\n",
    "                continue\n",
    "\n",
    "            # If we got here, the record passed validation. Add it to the results.\n",
    "            result.append(self.transform_body(item, result))\n",
    "\n",
    "            # Clear the flag if we find a valid surface sample\n",
    "            if item.type == '21':\n",
    "                surface_nan = 0\n",
    "\n",
    "            # Update tracking variables\n",
    "            last_gph = item.gph\n",
    "            usable_count += 1\n",
    "\n",
    "        # Final validation\n",
    "        if usable_count >= self.min_usable and surface_nan == 0 and last_gph >= self.gph_top:\n",
    "            # Reject the entire obs if we don't have 20 valid samples, there is \n",
    "            # not a valid surface sample, or if the balloon didn't make it to 10k\n",
    "            # above the surface.\n",
    "            return result\n",
    "        else:\n",
    "            # Passed validation, return the results\n",
    "            return []\n",
    "\n",
    "    def transform_body(self, item: olieigra.BodyModel, agg: list[list[float]]):\n",
    "        \"\"\"Transform the body\"\"\"\n",
    "\n",
    "        agg[0].append(item.gph)\n",
    "        agg[1].append(item.pres / 100.0)\n",
    "        agg[2].append(item.temp / 10.0)\n",
    "        agg[3].append((item.temp - item.dpdp) / 10.0)\n",
    "\n",
    "        # Convert wind from degrees/m^s to u,v\n",
    "        wrad = math.radians(item.wdir)\n",
    "        agg[4].append(-item.wspd * math.sin(wrad) / 10.0)\n",
    "        agg[5].append(-item.wspd * math.cos(wrad) / 10.0)\n",
    "\n",
    "    def body_pivot(self, body: list[list[float]]) -> list[float]:\n",
    "        \"\"\"Pivot and interpolate the levels\"\"\"\n",
    "        return [np.interp(level, body[0], x)\n",
    "                for level in self.levels\n",
    "                for x in [body[0], body[1], body[2], body[3], body[4], body[5]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping USM00072249-data.txt. Destination file already exists.\n",
      "Skipping USM00072250-data.txt. Destination file already exists.\n",
      "Skipping USM00072251-data.txt. Destination file already exists.\n",
      "Skipping USM00072261-data.txt. Destination file already exists.\n",
      "Skipping USM00072265-data.txt. Destination file already exists.\n",
      "Skipping USM00072357-data.txt. Destination file already exists.\n",
      "Skipping USM00072363-data.txt. Destination file already exists.\n",
      "Skipping USM00072364-data.txt. Destination file already exists.\n",
      "Skipping USM00072440-data.txt. Destination file already exists.\n",
      "Skipping USM00072451-data.txt. Destination file already exists.\n",
      "Skipping USM00072456-data.txt. Destination file already exists.\n",
      "Skipping USM00072476-data.txt. Destination file already exists.\n",
      "Skipping USM00072558-data.txt. Destination file already exists.\n",
      "Skipping USM00072562-data.txt. Destination file already exists.\n",
      "Skipping USM00072645-data.txt. Destination file already exists.\n",
      "Skipping USM00072649-data.txt. Destination file already exists.\n",
      "Skipping USM00072659-data.txt. Destination file already exists.\n",
      "Skipping USM00072662-data.txt. Destination file already exists.\n",
      "Skipping USM00072747-data.txt. Destination file already exists.\n",
      "Skipping USM00072764-data.txt. Destination file already exists.\n",
      "Skipping USM00074455-data.txt. Destination file already exists.\n",
      "Skipping USM00074560-data.txt. Destination file already exists.\n",
      "Skipping USM00074646-data.txt. Destination file already exists.\n"
     ]
    }
   ],
   "source": [
    "callbacks = Gph20S10K(SILVER_GPH20S10K_PATH, datetime(2000, 1, 1))\n",
    "reader = olieigra.Reader(callbacks=callbacks)\n",
    "crawler = olieigra.Crawler(reader=reader)\n",
    "\n",
    "crawler.crawl(BRONZE_DATA_POR_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olieigra_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
