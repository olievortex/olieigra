{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Data for Analysis\n",
    "This notebook aggregates data to help analyze the igra data. The main goal is to figure out the granularity of the data in terms of geopotential height. In particular, I am interested in the number of observations in the troposphere, where most of the weather happens. I arbitrarily decided the \"top\" of the troposphere is 10km.\n",
    "\n",
    "For each observation, we filter out:\n",
    "- Records older than 1/1/2000\n",
    "- Non-pressure records\n",
    "- Records where an observation is missing or invalid\n",
    "- Observations where there isn't a valid surface record\n",
    "\n",
    "For each observation, we find:\n",
    "- Max Geopotential Height\n",
    "- Minimum Pressure\n",
    "- Total number of valid records in the observation\n",
    "- Number of valid records below 10km in geopotential height\n",
    "\n",
    "Update the following parameters in the first cell to accomodate your installation:\n",
    "\n",
    "- BRONZE_DATA_POR_PATH - The location of the raw data-por zip files\n",
    "- SILVER_QA_PATH - The location to save the transformed CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need to install olieigra, uncomment and execute this line. View the README in the project root for instructions\n",
    "# on how to build or download this file.\n",
    "#%pip install /lakehouse/default/Files/libs/olieigra-0.0.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from datetime import datetime\n",
    "import olieigra\n",
    "\n",
    "BRONZE_DATA_POR_PATH = '/usr/datalake/bronze/igra/data-por'\n",
    "SILVER_QA_PATH = '/usr/datalake/silver/igra/qa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the destination path exists\n",
    "os.makedirs(SILVER_QA_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The business logic is contained within this class\n",
    "\n",
    "class QualityAnalysis(olieigra.Callbacks):\n",
    "    def __init__(self, dst_path: str, min_effective_date: datetime):\n",
    "        super().__init__()\n",
    "        self.min_effective_date = min_effective_date\n",
    "        self.dst_path = dst_path\n",
    "        self.filename = ''\n",
    "        self.filtered = 0\n",
    "        self.writer = None\n",
    "        self.hout = ''\n",
    "\n",
    "    def start_file(self, filename: str) -> bool:\n",
    "        \"\"\"Decide if we want to process the file. If so, reset state and start writing to a\n",
    "        temporary file.\"\"\"\n",
    "\n",
    "        # An IGRA2 file should end with -data.txt\n",
    "        if not filename.endswith('-data.txt'):\n",
    "            print(f'Skipping {filename}. Not sure what to do with it.')\n",
    "            return False\n",
    "\n",
    "        # Set the desired destination filename\n",
    "        dst_filename = f'{self.dst_path}/{filename}'\n",
    "        dst_filename = dst_filename.replace(\"-data.txt\", \"-data-qa.csv\")\n",
    "\n",
    "        # Skip this file if it has already been processed\n",
    "        if os.path.exists(dst_filename):\n",
    "            print(f'Skipping {filename}. Destination file already exists.')\n",
    "            return False\n",
    "\n",
    "        # If we got here, we are going to process the file\n",
    "        print(f'Processing {filename}.')\n",
    "\n",
    "        # Write to a temp file\n",
    "        self.filename = dst_filename.replace('-data-qa.csv', '-data-qa.partial.csv')\n",
    "        self.writer = open(self.filename, 'w', encoding='UTF-8')\n",
    "\n",
    "        # Reset the filtered record count\n",
    "        self.filtered = 0\n",
    "        self.hout = ''\n",
    "\n",
    "        # Write the header row\n",
    "        self.writer.write('id,effective_date,hour,usable_10k,usable_all,max_gph,min_pa\\n')\n",
    "\n",
    "        # Tell olieigra to continue processing\n",
    "        return True\n",
    "    \n",
    "    def finish_file(self, headers: int, rows: int):\n",
    "        \"\"\"File processing is complete. Clean up and provide user feedback.\"\"\"\n",
    "\n",
    "        # Close the temporary file\n",
    "        self.writer.close()\n",
    "\n",
    "        # Rename the temporary file\n",
    "        dst_renamed = self.filename.replace('.partial.csv', '.csv')\n",
    "        os.rename(self.filename, dst_renamed)\n",
    "\n",
    "        # Calculate the number of records written\n",
    "        loaded = headers - self.filtered\n",
    "\n",
    "        # Provide feedback to the user\n",
    "        print(f\" Read {headers} headers, {rows} lines. Filtered {self.filtered}. \" +\n",
    "              f\"Wrote {loaded} records.\")\n",
    "\n",
    "    def parse_header(self, header: olieigra.HeaderModel) -> bool:\n",
    "        \"\"\"Transform the header record and start writing a record\"\"\"\n",
    "\n",
    "        # Combine the separate fields into a date\n",
    "        effective_date = datetime(header.year, header.month, header.day)\n",
    "\n",
    "        # Skip the record if it is too old\n",
    "        if effective_date < self.min_effective_date:\n",
    "            self.filtered += 1\n",
    "            return False\n",
    "\n",
    "        # We may not write the row, so save the header columns in a variable for now\n",
    "        self.hout = f'{header.id},{effective_date:%Y-%m-%d},{header.hour}'\n",
    "\n",
    "        # Tell olieigra to process the body associated with this header\n",
    "        return True\n",
    "    \n",
    "    def parse_body(self, body: list[olieigra.BodyModel]):\n",
    "        \"\"\"Perform some analytics and finish writing a record\"\"\"\n",
    "\n",
    "        # Initialize\n",
    "        usable_10k = 0\n",
    "        usable_all = 0\n",
    "        surface_nan = 1\n",
    "        max_gph = -1\n",
    "        min_pa = 99999999\n",
    "\n",
    "        # Iterate through each record in the body\n",
    "        for item in body:\n",
    "            # We don't care about non-pressure records\n",
    "            if item.type[0] == '3':\n",
    "                continue\n",
    "\n",
    "            # We don't care about records that contain a NaN value\n",
    "            if math.isnan(item.dpdp) | math.isnan(item.rh) | math.isnan(item.temp) | \\\n",
    "                    math.isnan(item.wdir) | math.isnan(item.wspd) | math.isnan(item.gph):\n",
    "                continue\n",
    "\n",
    "            # Find the highest gph\n",
    "            if item.gph > max_gph:\n",
    "                max_gph = item.gph\n",
    "\n",
    "            # Find the lowest pa\n",
    "            if item.pres < min_pa:\n",
    "                min_pa = item.pres\n",
    "\n",
    "            # This is a usable record\n",
    "            usable_all += 1\n",
    "\n",
    "            # We don't care about records higher than 10km\n",
    "            if item.gph > 10000:\n",
    "                continue\n",
    "\n",
    "            # Flag that we found a valid surface record\n",
    "            if item.type == '21':\n",
    "                surface_nan = 0\n",
    "\n",
    "            # If we got here, the record is usable and lower then 10k height\n",
    "            usable_10k += 1\n",
    "\n",
    "        # Write record if there was a valid surface record\n",
    "        if surface_nan == 0:\n",
    "            self.writer.write(f'{self.hout},{usable_10k},{usable_all},{max_gph},{min_pa}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing USM00072249-data.txt.\n",
      " Read 50909 headers, 4737531 lines. Filtered 31519. Wrote 19390 records.\n",
      "Processing USM00072250-data.txt.\n",
      " Read 78580 headers, 5759477 lines. Filtered 59498. Wrote 19082 records.\n",
      "Processing USM00072251-data.txt.\n",
      " Read 35512 headers, 5007828 lines. Filtered 16310. Wrote 19202 records.\n",
      "Processing USM00072261-data.txt.\n",
      " Read 54774 headers, 5410571 lines. Filtered 35986. Wrote 18788 records.\n",
      "Processing USM00072265-data.txt.\n",
      " Read 63936 headers, 5382956 lines. Filtered 44950. Wrote 18986 records.\n",
      "Processing USM00072357-data.txt.\n",
      " Read 27575 headers, 4371655 lines. Filtered 8706. Wrote 18869 records.\n",
      "Processing USM00072363-data.txt.\n",
      " Read 78108 headers, 5394511 lines. Filtered 59076. Wrote 19032 records.\n",
      "Processing USM00072364-data.txt.\n",
      " Read 76969 headers, 5403556 lines. Filtered 58112. Wrote 18857 records.\n",
      "Processing USM00072440-data.txt.\n",
      " Read 33986 headers, 4363593 lines. Filtered 14599. Wrote 19387 records.\n",
      "Processing USM00072451-data.txt.\n",
      " Read 66855 headers, 5124222 lines. Filtered 47852. Wrote 19003 records.\n",
      "Processing USM00072456-data.txt.\n",
      " Read 55472 headers, 5422176 lines. Filtered 36402. Wrote 19070 records.\n",
      "Processing USM00072476-data.txt.\n",
      " Read 70156 headers, 5199227 lines. Filtered 51675. Wrote 18481 records.\n",
      "Processing USM00072558-data.txt.\n",
      " Read 84756 headers, 5545662 lines. Filtered 65841. Wrote 18915 records.\n",
      "Processing USM00072562-data.txt.\n",
      " Read 77468 headers, 5447033 lines. Filtered 58738. Wrote 18730 records.\n",
      "Processing USM00072645-data.txt.\n",
      " Read 63185 headers, 5289872 lines. Filtered 44399. Wrote 18786 records.\n",
      "Processing USM00072649-data.txt.\n",
      " Read 28130 headers, 4133846 lines. Filtered 9224. Wrote 18906 records.\n",
      "Processing USM00072659-data.txt.\n",
      " Read 22598 headers, 4145048 lines. Filtered 3693. Wrote 18905 records.\n",
      "Processing USM00072662-data.txt.\n",
      " Read 70545 headers, 5356710 lines. Filtered 51686. Wrote 18859 records.\n",
      "Processing USM00072747-data.txt.\n",
      " Read 62689 headers, 5283897 lines. Filtered 44039. Wrote 18650 records.\n",
      "Processing USM00072764-data.txt.\n",
      " Read 80495 headers, 5524320 lines. Filtered 61574. Wrote 18921 records.\n",
      "Processing USM00074455-data.txt.\n",
      " Read 24614 headers, 4320985 lines. Filtered 5560. Wrote 19054 records.\n",
      "Processing USM00074560-data.txt.\n",
      " Read 22781 headers, 4253432 lines. Filtered 3539. Wrote 19242 records.\n",
      "Processing USM00074646-data.txt.\n",
      " Read 26489 headers, 2488583 lines. Filtered 0. Wrote 26489 records.\n"
     ]
    }
   ],
   "source": [
    "# Set up for processing\n",
    "callbacks = QualityAnalysis(SILVER_QA_PATH, datetime(2000, 1, 1))\n",
    "reader = olieigra.Reader(callbacks=callbacks)\n",
    "crawler = olieigra.Crawler(reader=reader)\n",
    "\n",
    "# Crawl and process files\n",
    "crawler.crawl(BRONZE_DATA_POR_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olieigra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
