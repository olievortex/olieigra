{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33560cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import glob\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import Module, Sequential, Linear, Tanh, MSELoss\n",
    "\n",
    "IGRA_PATH = '/usr/datalake/silver/igra/gph20s10k'\n",
    "STATION_LIST = '/usr/datalake/silver/igra/doc/igra2-station-list.csv'\n",
    "ARTIFACTS_PATH = '/usr/datalake/silver/stormevents/csvfiles/igra_maidenhead'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8256237",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Sequential(\n",
    "            Linear(105, 80),\n",
    "            Tanh(),\n",
    "            Linear(80, 60),\n",
    "            Tanh(),\n",
    "            Linear(60, 40),\n",
    "            Tanh(),\n",
    "            Linear(40, 20)\n",
    "        )\n",
    "\n",
    "        self.decoder = Sequential(\n",
    "            Linear(20, 40),\n",
    "            Tanh(),\n",
    "            Linear(40, 60),\n",
    "            Tanh(),\n",
    "            Linear(60, 80),\n",
    "            Tanh(),\n",
    "            Linear(80, 105)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b2073f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class olie_igra_trainer:\n",
    "    batch_size = 256\n",
    "    epochs = 1024\n",
    "    learning_rate = 0.005\n",
    "    learning_rate_gamma = 0.99\n",
    "    lambda_param = .0001\n",
    "\n",
    "    def __init__(self, igra_path: str, artifact_path: str, station_id: str, model):\n",
    "        self.igra_path = igra_path\n",
    "        self.station_id = station_id\n",
    "        self.artifact_path = artifact_path\n",
    "        self.model = model\n",
    "\n",
    "    def load_transform_dataset(self):\n",
    "        X = pd.read_csv(f'{self.igra_path}/{self.station_id}-data-gph20s10k.csv')\n",
    "\n",
    "        # Remove irrelevant data\n",
    "        X = X[X['hour'] == 12]\n",
    "        X = X.drop(['id', 'effective_date', 'hour', 'day_num', '0_gph',\n",
    "                    '1_gph', '2_gph', '3_gph', '4_gph', '5_gph',\n",
    "                    '6_gph', '7_gph', '8_gph', '9_gph', '10_gph',\n",
    "                    '11_gph', '12_gph', '13_gph', '14_gph', '15_gph',\n",
    "                    '16_gph', '17_gph', '18_gph', '19_gph', '20_gph'\n",
    "                    ], axis=1)\n",
    "        if X.shape[0] == 0:\n",
    "            return False\n",
    "        \n",
    "        # Scale the X dataset\n",
    "        ss = PowerTransformer()\n",
    "        X = ss.fit_transform(X)\n",
    "\n",
    "        # Save the transform\n",
    "        os.makedirs(self.artifact_path, exist_ok=True)\n",
    "        with open(f'{self.artifact_path}/{self.station_id}_scaler.pkl', 'wb') as f:\n",
    "            pickle.dump(ss, f)\n",
    "        \n",
    "        train, test = train_test_split(X, test_size=0.2)\n",
    "        self.x_train = torch.from_numpy(train).float().cuda()\n",
    "        self.x_test = torch.from_numpy(test).float().cuda()\n",
    "        self.n_batches = self.x_train.size()[0] // self.batch_size\n",
    "\n",
    "        print (f\"Station ID: {self.station_id}, Training size: {self.x_train.size()[0]:,}, Predict size: {self.x_test.size()[0]:,}, Feature count: {self.x_train.size()[1]}, Number of batches: {self.n_batches}\")\n",
    "\n",
    "        return True\n",
    "    \n",
    "    def train(self, inputs, labels) -> float:\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # Calculate error\n",
    "        logits = self.model(inputs)\n",
    "        cost = self.loss_function(logits, labels)\n",
    "\n",
    "        # Back propagation\n",
    "        cost.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return float(cost.item())\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # Calculate error\n",
    "        logits = self.model(inputs).clone().detach()\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    def r2_score_manual(self, preds, target):\n",
    "        target_mean = torch.mean(target)\n",
    "        ss_tot = torch.sum((target - target_mean) ** 2) # Total sum of squares\n",
    "        ss_res = torch.sum((target - preds) ** 2)       # Residual sum of squares\n",
    "        r2 = 1 - (ss_res / ss_tot)\n",
    "        \n",
    "        return float(r2.item())\n",
    "    \n",
    "    def output_progress(self, epoch: int, cost: float):\n",
    "        preds = self.predict(self.x_test)\n",
    "        acc = self.r2_score_manual(self.x_test, preds)\n",
    "        print(f\"Epoch: {epoch+1}, cost: {cost / self.n_batches:.4f}, acc: {acc:.3f}, lr: {self.scheduler.get_last_lr()[0]:.2e}\\r\", end=\"\")\n",
    "        \n",
    "    def train_orch(self):\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), self.learning_rate)\n",
    "        self.loss_function = MSELoss()\n",
    "        self.scheduler = torch.optim.lr_scheduler.ExponentialLR(self.optimizer, gamma=self.learning_rate_gamma)\n",
    "    \n",
    "        for epoch in range(self.epochs):\n",
    "            cost = 0\n",
    "            loader = DataLoader(dataset = self.x_train, batch_size = self.batch_size, shuffle = True)\n",
    "\n",
    "            for batch in loader:\n",
    "                cost += self.train(batch, batch)\n",
    "\n",
    "            self.scheduler.step()\n",
    "\n",
    "            if epoch % 32 == 0:\n",
    "                self.output_progress(epoch, cost)\n",
    "        \n",
    "        self.output_progress(epoch, cost)\n",
    "        print()\n",
    "\n",
    "    def save_weights(self):\n",
    "        torch.save(self.model.state_dict(), f'{self.artifact_path}/{self.station_id}_fnn.pt')\n",
    "\n",
    "    def exists_weights(self):\n",
    "        return os.path.exists(f'{self.artifact_path}/{self.station_id}_fnn.pt')\n",
    "\n",
    "    def dispose(self):\n",
    "        del self.x_train\n",
    "        del self.x_test\n",
    "        del self.optimizer\n",
    "        del self.loss_function\n",
    "        del self.scheduler\n",
    "        del self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "610544ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_station(station_id: str):\n",
    "    model = AutoEncoder().cuda()\n",
    "    train = olie_igra_trainer(IGRA_PATH, ARTIFACTS_PATH, station_id, model)\n",
    "\n",
    "    if train.exists_weights():\n",
    "        print(f\"Station {station_id} already processed\")\n",
    "        return\n",
    "\n",
    "    result = train.load_transform_dataset()\n",
    "    if not result:\n",
    "        print(f\"Station {station_id} has zero usable rows\")\n",
    "        return\n",
    "    \n",
    "    train.train_orch()\n",
    "    train.save_weights()\n",
    "    train.dispose()\n",
    "\n",
    "    del train\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d05eba1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Station BBM00078954 already processed\n",
      "Station BDM00078016 has zero usable rows\n",
      "Station ID: BHM00078583, Training size: 1,558, Predict size: 390, Feature count: 105, Number of batches: 6\n",
      "Epoch: 1024, cost: 0.1046, acc: 0.870, lr: 1.70e-07\n",
      "Station BRM00082022 has zero usable rows\n",
      "Station BRM00082026 has zero usable rows\n",
      "Station BRM00082099 has zero usable rows\n",
      "Station CAM00071043 has zero usable rows\n",
      "Station CAM00071081 has zero usable rows\n",
      "Station CAM00071082 has zero usable rows\n",
      "Station CAM00071109 has zero usable rows\n",
      "Station CAM00071119 has zero usable rows\n",
      "Station CAM00071603 has zero usable rows\n",
      "Station CAM00071701 has zero usable rows\n",
      "Station CAM00071722 has zero usable rows\n",
      "Station CAM00071802 has zero usable rows\n",
      "Station CAM00071811 has zero usable rows\n",
      "Station CAM00071815 has zero usable rows\n",
      "Station CAM00071816 has zero usable rows\n",
      "Station CAM00071823 has zero usable rows\n",
      "Station CAM00071843 has zero usable rows\n",
      "Station CAM00071845 has zero usable rows\n",
      "Station CAM00071867 has zero usable rows\n",
      "Station CAM00071906 has zero usable rows\n",
      "Station CAM00071907 has zero usable rows\n",
      "Station CAM00071908 has zero usable rows\n",
      "Station CAM00071909 has zero usable rows\n",
      "Station CAM00071913 has zero usable rows\n",
      "Station CAM00071917 has zero usable rows\n",
      "Station CAM00071924 has zero usable rows\n",
      "Station CAM00071925 has zero usable rows\n",
      "Station CAM00071926 has zero usable rows\n",
      "Station CAM00071934 has zero usable rows\n",
      "Station CAM00071945 has zero usable rows\n",
      "Station CAM00071957 has zero usable rows\n",
      "Station CAM00071964 has zero usable rows\n",
      "Station CAM00073033 has zero usable rows\n",
      "Station CAM00073111 has zero usable rows\n",
      "Station ID: CJM00078384, Training size: 261, Predict size: 66, Feature count: 105, Number of batches: 1\n",
      "Epoch: 1024, cost: 0.5150, acc: 0.491, lr: 1.70e-07\n",
      "Station ID: COM00080001, Training size: 386, Predict size: 97, Feature count: 105, Number of batches: 1\n",
      "Epoch: 1024, cost: 0.2687, acc: 0.721, lr: 1.70e-07\n",
      "Station COM00080028 has zero usable rows\n",
      "Station COM00080094 has zero usable rows\n",
      "Station COM00080222 has zero usable rows\n",
      "Station COM00080259 has zero usable rows\n",
      "Station ID: DRM00078486, Training size: 897, Predict size: 225, Feature count: 105, Number of batches: 3\n",
      "Epoch: 1024, cost: 0.1893, acc: 0.618, lr: 1.70e-07\n",
      "Station FGM00081405 has zero usable rows\n",
      "Station GLM00004220 has zero usable rows\n",
      "Station GLM00004270 has zero usable rows\n",
      "Station GLM00004360 has zero usable rows\n",
      "Station GLM00004417 has zero usable rows\n",
      "Station GPM00078897 has zero usable rows\n",
      "Station ID: JMM00078397, Training size: 483, Predict size: 121, Feature count: 105, Number of batches: 1\n",
      "Epoch: 1024, cost: 0.3369, acc: 0.669, lr: 1.70e-07\n",
      "Station MXM00076256 has zero usable rows\n",
      "Station MXM00076394 has zero usable rows\n",
      "Station MXM00076405 has zero usable rows\n",
      "Station MXM00076458 has zero usable rows\n",
      "Station MXM00076526 has zero usable rows\n",
      "Station MXM00076595 has zero usable rows\n",
      "Station MXM00076612 has zero usable rows\n",
      "Station MXM00076644 has zero usable rows\n",
      "Station MXM00076654 has zero usable rows\n",
      "Station MXM00076679 has zero usable rows\n",
      "Station MXM00076692 has zero usable rows\n",
      "Station MXM00076743 has zero usable rows\n",
      "Station MXM00076805 has zero usable rows\n",
      "Station MXM00076903 has zero usable rows\n",
      "Station ID: NNM00078866, Training size: 774, Predict size: 194, Feature count: 105, Number of batches: 3\n",
      "Epoch: 1024, cost: 0.1953, acc: 0.749, lr: 1.70e-07\n",
      "Station PMM00078807 has zero usable rows\n",
      "Station ID: RQM00078526, Training size: 7,150, Predict size: 1,788, Feature count: 105, Number of batches: 27\n",
      "Epoch: 1024, cost: 0.0812, acc: 0.899, lr: 1.70e-07\n",
      "Station ID: TDM00078970, Training size: 1,566, Predict size: 392, Feature count: 105, Number of batches: 6\n",
      "Epoch: 1024, cost: 0.1793, acc: 0.682, lr: 1.70e-07\n",
      "Station ID: UCM00078988, Training size: 2,130, Predict size: 533, Feature count: 105, Number of batches: 8\n",
      "Epoch: 1024, cost: 0.1318, acc: 0.674, lr: 1.70e-07\n",
      "Station ID: USM00070026, Training size: 5,987, Predict size: 1,497, Feature count: 105, Number of batches: 23\n",
      "Epoch: 1024, cost: 0.0236, acc: 0.975, lr: 1.70e-07\n",
      "Station ID: USM00070133, Training size: 6,246, Predict size: 1,562, Feature count: 105, Number of batches: 24\n",
      "Epoch: 1024, cost: 0.0235, acc: 0.976, lr: 1.70e-07\n",
      "Station ID: USM00070200, Training size: 6,703, Predict size: 1,676, Feature count: 105, Number of batches: 26\n",
      "Epoch: 1024, cost: 0.0232, acc: 0.977, lr: 1.70e-07\n",
      "Station ID: USM00070219, Training size: 6,796, Predict size: 1,700, Feature count: 105, Number of batches: 26\n",
      "Epoch: 1024, cost: 0.0236, acc: 0.976, lr: 1.70e-07\n",
      "Station ID: USM00070231, Training size: 6,568, Predict size: 1,642, Feature count: 105, Number of batches: 25\n",
      "Epoch: 1024, cost: 0.0288, acc: 0.970, lr: 1.70e-07\n",
      "Station ID: USM00070261, Training size: 6,425, Predict size: 1,607, Feature count: 105, Number of batches: 25\n",
      "Epoch: 1024, cost: 0.0287, acc: 0.969, lr: 1.70e-07\n",
      "Station ID: USM00070273, Training size: 7,048, Predict size: 1,763, Feature count: 105, Number of batches: 27\n",
      "Epoch: 1024, cost: 0.0288, acc: 0.971, lr: 1.70e-07\n",
      "Station ID: USM00070308, Training size: 6,550, Predict size: 1,638, Feature count: 105, Number of batches: 25\n",
      "Epoch: 1024, cost: 0.0223, acc: 0.978, lr: 1.70e-07\n",
      "Station ID: USM00070316, Training size: 6,715, Predict size: 1,679, Feature count: 105, Number of batches: 26\n",
      "Epoch: 1024, cost: 0.0244, acc: 0.974, lr: 1.70e-07\n",
      "Station ID: USM00070326, Training size: 6,915, Predict size: 1,729, Feature count: 105, Number of batches: 27\n",
      "Epoch: 1024, cost: 0.0278, acc: 0.971, lr: 1.70e-07\n",
      "Station ID: USM00070350, Training size: 6,340, Predict size: 1,585, Feature count: 105, Number of batches: 24\n",
      "Epoch: 1024, cost: 0.0248, acc: 0.975, lr: 1.70e-07\n",
      "Station ID: USM00070361, Training size: 6,500, Predict size: 1,626, Feature count: 105, Number of batches: 25\n",
      "Epoch: 1024, cost: 0.0292, acc: 0.968, lr: 1.70e-07\n",
      "Station ID: USM00070398, Training size: 6,219, Predict size: 1,555, Feature count: 105, Number of batches: 24\n",
      "Epoch: 1024, cost: 0.0283, acc: 0.970, lr: 1.70e-07\n",
      "Station ID: USM00072201, Training size: 7,288, Predict size: 1,822, Feature count: 105, Number of batches: 28\n",
      "Epoch: 1024, cost: 0.0562, acc: 0.935, lr: 1.70e-07\n",
      "Station ID: USM00072202, Training size: 6,994, Predict size: 1,749, Feature count: 105, Number of batches: 27\n",
      "Epoch: 1024, cost: 0.0569, acc: 0.941, lr: 1.70e-07\n",
      "Station ID: USM00072206, Training size: 6,931, Predict size: 1,733, Feature count: 105, Number of batches: 27\n",
      "Epoch: 1024, cost: 0.0422, acc: 0.946, lr: 1.70e-07\n",
      "Station ID: USM00072208, Training size: 5,909, Predict size: 1,478, Feature count: 105, Number of batches: 23\n",
      "Epoch: 1024, cost: 0.0379, acc: 0.960, lr: 1.70e-07\n",
      "Station ID: USM00072210, Training size: 6,688, Predict size: 1,672, Feature count: 105, Number of batches: 26\n",
      "Epoch: 1024, cost: 0.0468, acc: 0.939, lr: 1.70e-07\n",
      "Station ID: USM00072215, Training size: 6,485, Predict size: 1,622, Feature count: 105, Number of batches: 25\n",
      "Epoch: 1024, cost: 0.0370, acc: 0.960, lr: 1.70e-07\n",
      "Station USM00072221 has zero usable rows\n",
      "Station ID: USM00072230, Training size: 6,482, Predict size: 1,621, Feature count: 105, Number of batches: 25\n",
      "Epoch: 1024, cost: 0.0365, acc: 0.961, lr: 1.70e-07\n",
      "Station ID: USM00072233, Training size: 7,151, Predict size: 1,788, Feature count: 105, Number of batches: 27\n",
      "Epoch: 1024, cost: 0.0400, acc: 0.957, lr: 1.70e-07\n",
      "Station ID: USM00072235, Training size: 7,036, Predict size: 1,760, Feature count: 105, Number of batches: 27\n",
      "Epoch: 1024, cost: 0.0388, acc: 0.950, lr: 1.70e-07\n",
      "Station ID: USM00072240, Training size: 6,723, Predict size: 1,681, Feature count: 105, Number of batches: 26\n",
      "Epoch: 1024, cost: 0.0396, acc: 0.953, lr: 1.70e-07\n",
      "Station ID: USM00072248, Training size: 6,595, Predict size: 1,649, Feature count: 105, Number of batches: 25\n",
      "Epoch: 1024, cost: 0.0393, acc: 0.955, lr: 1.70e-07\n",
      "Station ID: USM00072249, Training size: 7,204, Predict size: 1,802, Feature count: 105, Number of batches: 28\n",
      "Epoch: 1024, cost: 0.0374, acc: 0.961, lr: 1.70e-07\n",
      "Station ID: USM00072250, Training size: 7,121, Predict size: 1,781, Feature count: 105, Number of batches: 27\n",
      "Epoch: 1024, cost: 0.0521, acc: 0.935, lr: 1.70e-07\n",
      "Station ID: USM00072251, Training size: 6,824, Predict size: 1,707, Feature count: 105, Number of batches: 26\n",
      "Epoch: 1024, cost: 0.0487, acc: 0.949, lr: 1.70e-07\n",
      "Station ID: USM00072261, Training size: 6,644, Predict size: 1,661, Feature count: 105, Number of batches: 25\n",
      "Epoch: 1024, cost: 0.0471, acc: 0.942, lr: 1.70e-07\n",
      "Station ID: USM00072265, Training size: 6,477, Predict size: 1,620, Feature count: 105, Number of batches: 25\n",
      "Epoch: 1024, cost: 0.0359, acc: 0.964, lr: 1.70e-07\n",
      "Station ID: USM00072274, Training size: 6,752, Predict size: 1,689, Feature count: 105, Number of batches: 26\n",
      "Epoch: 1024, cost: 0.0343, acc: 0.963, lr: 1.70e-07\n",
      "Station ID: USM00072293, Training size: 6,653, Predict size: 1,664, Feature count: 105, Number of batches: 25\n",
      "Epoch: 1024, cost: 0.0457, acc: 0.949, lr: 1.70e-07\n",
      "Station ID: USM00072305, Training size: 6,564, Predict size: 1,642, Feature count: 105, Number of batches: 25\n",
      "Epoch: 1024, cost: 0.0352, acc: 0.963, lr: 1.70e-07\n",
      "Station ID: USM00072317, Training size: 6,475, Predict size: 1,619, Feature count: 105, Number of batches: 25\n",
      "Epoch: 1024, cost: 0.0322, acc: 0.955, lr: 1.70e-07\n",
      "Station ID: USM00072318, Training size: 6,588, Predict size: 1,648, Feature count: 105, Number of batches: 25\n",
      "Epoch: 1024, cost: 0.0283, acc: 0.970, lr: 1.70e-07\n",
      "Station ID: USM00072327, Training size: 6,660, Predict size: 1,666, Feature count: 105, Number of batches: 26\n",
      "Epoch: 1024, cost: 0.0339, acc: 0.962, lr: 1.70e-07\n",
      "Station ID: USM00072340, Training size: 6,009, Predict size: 1,503, Feature count: 105, Number of batches: 23\n",
      "Epoch: 1024, cost: 0.0363, acc: 0.961, lr: 1.70e-07\n",
      "Station ID: USM00072357, Training size: 6,316, Predict size: 1,579, Feature count: 105, Number of batches: 24\n",
      "Epoch: 1024, cost: 0.0350, acc: 0.961, lr: 1.70e-07\n",
      "Station ID: USM00072363, Training size: 6,326, Predict size: 1,582, Feature count: 105, Number of batches: 24\n",
      "Epoch: 1024, cost: 0.0309, acc: 0.968, lr: 1.70e-07\n",
      "Station ID: USM00072364, Training size: 6,829, Predict size: 1,708, Feature count: 105, Number of batches: 26\n",
      "Epoch: 1024, cost: 0.0282, acc: 0.968, lr: 1.70e-07\n",
      "Station ID: USM00072365, Training size: 6,227, Predict size: 1,557, Feature count: 105, Number of batches: 24\n",
      "Epoch: 1024, cost: 0.0250, acc: 0.975, lr: 1.70e-07\n",
      "Station ID: USM00072376, Training size: 6,391, Predict size: 1,598, Feature count: 105, Number of batches: 24\n",
      "Epoch: 1024, cost: 0.0251, acc: 0.973, lr: 1.70e-07\n",
      "Station ID: USM00072381, Training size: 256, Predict size: 64, Feature count: 105, Number of batches: 1\n",
      "Epoch: 1024, cost: 0.1449, acc: 0.760, lr: 1.70e-07\n",
      "Station ID: USM00072388, Training size: 3,785, Predict size: 947, Feature count: 105, Number of batches: 14\n",
      "Epoch: 1024, cost: 0.0392, acc: 0.958, lr: 1.70e-07\n",
      "Station USM00072391 has zero usable rows\n",
      "Station USM00072393 has zero usable rows\n",
      "Station ID: USM00072402, Training size: 4,033, Predict size: 1,009, Feature count: 105, Number of batches: 15\n",
      "Epoch: 1024, cost: 0.0367, acc: 0.963, lr: 1.70e-07\n",
      "Station ID: USM00072403, Training size: 7,028, Predict size: 1,758, Feature count: 105, Number of batches: 27\n",
      "Epoch: 1024, cost: 0.0312, acc: 0.966, lr: 1.70e-07\n",
      "Station ID: USM00072426, Training size: 6,635, Predict size: 1,659, Feature count: 105, Number of batches: 25\n",
      "Epoch: 1024, cost: 0.0318, acc: 0.966, lr: 1.70e-07\n",
      "Station ID: USM00072440, Training size: 6,615, Predict size: 1,654, Feature count: 105, Number of batches: 25\n",
      "Epoch: 1024, cost: 0.0321, acc: 0.967, lr: 1.70e-07\n",
      "Station ID: USM00072451, Training size: 6,975, Predict size: 1,744, Feature count: 105, Number of batches: 27\n",
      "Epoch: 1024, cost: 0.0299, acc: 0.969, lr: 1.70e-07\n",
      "Station ID: USM00072456, Training size: 6,792, Predict size: 1,699, Feature count: 105, Number of batches: 26\n",
      "Epoch: 1024, cost: 0.0311, acc: 0.965, lr: 1.70e-07\n",
      "Station ID: USM00072476, Training size: 6,497, Predict size: 1,625, Feature count: 105, Number of batches: 25\n",
      "Epoch: 1024, cost: 0.0279, acc: 0.971, lr: 1.70e-07\n",
      "Station ID: USM00072489, Training size: 6,730, Predict size: 1,683, Feature count: 105, Number of batches: 26\n",
      "Epoch: 1024, cost: 0.0277, acc: 0.972, lr: 1.70e-07\n",
      "Station ID: USM00072493, Training size: 6,921, Predict size: 1,731, Feature count: 105, Number of batches: 27\n",
      "Epoch: 1024, cost: 0.0428, acc: 0.955, lr: 1.70e-07\n",
      "Station ID: USM00072501, Training size: 6,727, Predict size: 1,682, Feature count: 105, Number of batches: 26\n",
      "Epoch: 1024, cost: 0.0283, acc: 0.971, lr: 1.70e-07\n",
      "Station ID: USM00072518, Training size: 5,707, Predict size: 1,427, Feature count: 105, Number of batches: 22\n",
      "Epoch: 1024, cost: 0.0293, acc: 0.970, lr: 1.70e-07\n",
      "Station ID: USM00072520, Training size: 6,896, Predict size: 1,725, Feature count: 105, Number of batches: 26\n",
      "Epoch: 1024, cost: 0.0286, acc: 0.970, lr: 1.70e-07\n",
      "Station ID: USM00072528, Training size: 6,741, Predict size: 1,686, Feature count: 105, Number of batches: 26\n",
      "Epoch: 1024, cost: 0.0284, acc: 0.970, lr: 1.70e-07\n",
      "Station ID: USM00072558, Training size: 6,518, Predict size: 1,630, Feature count: 105, Number of batches: 25\n",
      "Epoch: 1024, cost: 0.0299, acc: 0.969, lr: 1.70e-07\n",
      "Station ID: USM00072562, Training size: 6,438, Predict size: 1,610, Feature count: 105, Number of batches: 25\n",
      "Epoch: 1024, cost: 0.0294, acc: 0.968, lr: 1.70e-07\n",
      "Station ID: USM00072572, Training size: 6,666, Predict size: 1,667, Feature count: 105, Number of batches: 26\n",
      "Epoch: 1024, cost: 0.0298, acc: 0.970, lr: 1.70e-07\n",
      "Station ID: USM00072582, Training size: 6,332, Predict size: 1,584, Feature count: 105, Number of batches: 24\n",
      "Epoch: 1024, cost: 0.0260, acc: 0.973, lr: 1.70e-07\n",
      "Station ID: USM00072597, Training size: 6,960, Predict size: 1,741, Feature count: 105, Number of batches: 27\n",
      "Epoch: 1024, cost: 0.0378, acc: 0.961, lr: 1.70e-07\n",
      "Station ID: USM00072632, Training size: 7,047, Predict size: 1,762, Feature count: 105, Number of batches: 27\n",
      "Epoch: 1024, cost: 0.0269, acc: 0.973, lr: 1.70e-07\n",
      "Station ID: USM00072634, Training size: 6,836, Predict size: 1,709, Feature count: 105, Number of batches: 26\n",
      "Epoch: 1024, cost: 0.0260, acc: 0.974, lr: 1.70e-07\n",
      "Station ID: USM00072645, Training size: 6,607, Predict size: 1,652, Feature count: 105, Number of batches: 25\n",
      "Epoch: 1024, cost: 0.0297, acc: 0.969, lr: 1.70e-07\n",
      "Station ID: USM00072649, Training size: 6,695, Predict size: 1,674, Feature count: 105, Number of batches: 26\n",
      "Epoch: 1024, cost: 0.0296, acc: 0.969, lr: 1.70e-07\n",
      "Station ID: USM00072659, Training size: 6,845, Predict size: 1,712, Feature count: 105, Number of batches: 26\n",
      "Epoch: 1024, cost: 0.0281, acc: 0.972, lr: 1.70e-07\n",
      "Station ID: USM00072662, Training size: 6,660, Predict size: 1,665, Feature count: 105, Number of batches: 26\n",
      "Epoch: 1024, cost: 0.0305, acc: 0.969, lr: 1.70e-07\n",
      "Station ID: USM00072672, Training size: 6,503, Predict size: 1,626, Feature count: 105, Number of batches: 25\n",
      "Epoch: 1024, cost: 0.0256, acc: 0.970, lr: 1.70e-07\n",
      "Station ID: USM00072681, Training size: 6,947, Predict size: 1,737, Feature count: 105, Number of batches: 27\n",
      "Epoch: 1024, cost: 0.0312, acc: 0.966, lr: 1.70e-07\n",
      "Station ID: USM00072694, Training size: 6,719, Predict size: 1,680, Feature count: 105, Number of batches: 26\n",
      "Epoch: 1024, cost: 0.0362, acc: 0.963, lr: 1.70e-07\n",
      "Station ID: USM00072712, Training size: 6,647, Predict size: 1,662, Feature count: 105, Number of batches: 25\n",
      "Epoch: 1024, cost: 0.0265, acc: 0.972, lr: 1.70e-07\n",
      "Station ID: USM00072747, Training size: 6,450, Predict size: 1,613, Feature count: 105, Number of batches: 25\n",
      "Epoch: 1024, cost: 0.0297, acc: 0.969, lr: 1.70e-07\n",
      "Station ID: USM00072764, Training size: 6,556, Predict size: 1,640, Feature count: 105, Number of batches: 25\n",
      "Epoch: 1024, cost: 0.0289, acc: 0.971, lr: 1.70e-07\n",
      "Station ID: USM00072768, Training size: 6,673, Predict size: 1,669, Feature count: 105, Number of batches: 26\n",
      "Epoch: 1024, cost: 0.0290, acc: 0.970, lr: 1.70e-07\n",
      "Station ID: USM00072776, Training size: 6,554, Predict size: 1,639, Feature count: 105, Number of batches: 25\n",
      "Epoch: 1024, cost: 0.0250, acc: 0.974, lr: 1.70e-07\n",
      "Station ID: USM00072786, Training size: 6,444, Predict size: 1,611, Feature count: 105, Number of batches: 25\n",
      "Epoch: 1024, cost: 0.0309, acc: 0.965, lr: 1.70e-07\n",
      "Station ID: USM00072797, Training size: 6,714, Predict size: 1,679, Feature count: 105, Number of batches: 26\n",
      "Epoch: 1024, cost: 0.0344, acc: 0.962, lr: 1.70e-07\n",
      "Station USM00074003 has zero usable rows\n",
      "Station USM00074004 has zero usable rows\n",
      "Station USM00074005 has zero usable rows\n",
      "Station ID: USM00074389, Training size: 6,719, Predict size: 1,680, Feature count: 105, Number of batches: 26\n",
      "Epoch: 1024, cost: 0.0271, acc: 0.971, lr: 1.70e-07\n",
      "Station ID: USM00074455, Training size: 6,655, Predict size: 1,664, Feature count: 105, Number of batches: 25\n",
      "Epoch: 1024, cost: 0.0285, acc: 0.969, lr: 1.70e-07\n",
      "Station ID: USM00074560, Training size: 6,572, Predict size: 1,644, Feature count: 105, Number of batches: 25\n",
      "Epoch: 1024, cost: 0.0295, acc: 0.970, lr: 1.70e-07\n",
      "Station USM00074612 has zero usable rows\n",
      "Station USM00074626 has zero usable rows\n",
      "Station USM00074646 has zero usable rows\n",
      "Station USM00074794 has zero usable rows\n",
      "Station ID: USM00091165, Training size: 7,198, Predict size: 1,800, Feature count: 105, Number of batches: 28\n",
      "Epoch: 1024, cost: 0.0687, acc: 0.910, lr: 1.70e-07\n",
      "Station ID: USM00091285, Training size: 6,992, Predict size: 1,748, Feature count: 105, Number of batches: 27\n",
      "Epoch: 1024, cost: 0.0860, acc: 0.886, lr: 1.70e-07\n"
     ]
    }
   ],
   "source": [
    "for filepath in glob.glob(f'{IGRA_PATH}/*-data-gph20s10k.csv'):\n",
    "    filename = Path(filepath).name\n",
    "    station_id = filename.split('-')[0]\n",
    "\n",
    "    process_station(station_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c147a075",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olieigra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
