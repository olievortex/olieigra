{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Data gph20s10k\n",
    "Transform the data-por observations into a format we can use for machine learning. The problem is the samples in an observation are not consistent between observations. The solution is to interpolate data into standardized levels. We split the samples into 20 levels between the surface and 10km. This allows all the observations to be consistent for a given station. Although the number of levels are the same between other stations, because the surface elevation is different, the actual altitudes of each slice will be different.\n",
    "\n",
    "Observations are also filtered on quality. If there are less than 20 samples without data problems between the surface and 10km geopotential height, the entire observation is rejected. If a surface sample has any invalid data, the entire observation is also rejected.\n",
    "\n",
    "Update the following parameters in the first cell to accomodate your installation:\n",
    "\n",
    "- BRONZE_DATA_POR_PATH - The location of the raw data-por zip files\n",
    "- SILVER_GPH20S10K_PATH - The location to save the transformed CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need to install olieigra, uncomment and execute this line. View the README in the project root for instructions\n",
    "# on how to build or download this file.\n",
    "#%pip install /lakehouse/default/Files/libs/olieigra-0.0.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import olieigra\n",
    "\n",
    "BRONZE_DATA_POR_PATH = '/usr/datalake/bronze/igra/data-por'\n",
    "SILVER_GPH20S10K_PATH = '/usr/datalake/silver/igra/gph20s10k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the destination path exists\n",
    "os.makedirs(SILVER_GPH20S10K_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The business logic is contained within this class\n",
    "\n",
    "class Gph20S10K(olieigra.Callbacks):\n",
    "    samples = 21\n",
    "    gph_top = 10000\n",
    "    min_usable = 20\n",
    "\n",
    "    def __init__(self, dst_path: str, min_effective_date: datetime):\n",
    "        super().__init__()\n",
    "        self.dst_path = dst_path\n",
    "        self.filename = ''\n",
    "        self.filtered = 0\n",
    "        self.rejected = 0\n",
    "        self.writer = None\n",
    "        self.min_effective_date = min_effective_date\n",
    "        self.hout = ''\n",
    "        self.levels = []\n",
    "\n",
    "    def start_file(self, filename: str) -> bool:\n",
    "        \"\"\"Decide if we want to process the file. If so, reset state and start writing to a\n",
    "        temporary file.\"\"\"\n",
    "\n",
    "        # An IGRA2 file should end with -data.txt\n",
    "        if not filename.endswith('-data.txt'):\n",
    "            print(f'Skipping {filename}. Not sure what to do with it.')\n",
    "            return False\n",
    "\n",
    "        # Set the desired destination filename\n",
    "        dst_filename = f'{self.dst_path}/{filename}'\n",
    "        dst_filename = dst_filename.replace(\"-data.txt\", \"-data-gph20s10k.csv\")\n",
    "\n",
    "        # Skip this file if it has already been processed\n",
    "        if os.path.exists(dst_filename):\n",
    "            print(f'Skipping {filename}. Destination file already exists.')\n",
    "            return False\n",
    "\n",
    "        # If we got here, we are going to process the file\n",
    "        print(f'Processing {filename}.')\n",
    "\n",
    "        # Write to a temp file\n",
    "        self.filename = dst_filename.replace('.csv', '.partial.csv')\n",
    "        self.writer = open(self.filename, 'w', encoding='UTF-8')\n",
    "        self.hout = ''\n",
    "\n",
    "        # Reset the record counts\n",
    "        self.filtered = 0\n",
    "        self.rejected = 0\n",
    "\n",
    "        # Write the header row\n",
    "        attr = ['gph','pres','temp','dp','u','v']\n",
    "        dynamic = ','.join([f'{level}_{x}'\n",
    "                            for level in range(self.samples)\n",
    "                            for x in attr])\n",
    "        self.writer.write(f\"id,effective_date,hour,day_num,{dynamic}\\n\")\n",
    "\n",
    "        # Tell olieigra to continue processing\n",
    "        return True\n",
    "\n",
    "    def finish_file(self, headers: int, rows: int):\n",
    "        \"\"\"Callback for when processing is complete\"\"\"\n",
    "\n",
    "        # Flush and close the temp file\n",
    "        self.writer.close()\n",
    "\n",
    "        # Rename it to the final filename\n",
    "        dst_renamed = self.filename.replace('.partial.csv', '.csv')\n",
    "        os.rename(self.filename, dst_renamed)\n",
    "\n",
    "        # Calculate the number of records written\n",
    "        loaded = headers - self.filtered - self.rejected\n",
    "\n",
    "        print(f\" Read {headers} headers, {rows} lines. Filtered {self.filtered}. \" +\n",
    "              f\"Rejected {self.rejected}. Wrote {loaded} records.\")\n",
    "\n",
    "    def parse_header(self, header: olieigra.HeaderModel):\n",
    "        \"\"\"Transform the header record\"\"\"\n",
    "\n",
    "        # Combine seperate fields into a datetime\n",
    "        effective_date = datetime(header.year, header.month, header.day)\n",
    "\n",
    "        # Filter out the observations that are too old\n",
    "        if effective_date < self.min_effective_date:\n",
    "            self.filtered += 1\n",
    "            return False\n",
    "\n",
    "        # We need some number that is analogous to the amount of sunlight and the season\n",
    "        day_num = -math.cos(math.radians(effective_date.timetuple().tm_yday))\n",
    "\n",
    "        # The observation may be rejected due to body data issues. Save the header values to\n",
    "        # a variable for now. The parse_body will write it to the file, if appropriate.\n",
    "        self.hout = f'{header.id},{effective_date:%Y-%m-%d},{header.hour},{day_num:.2f}'\n",
    "\n",
    "        # Continue the processing\n",
    "        return True\n",
    "\n",
    "    def parse_body(self, body: list[olieigra.BodyModel]):\n",
    "        \"\"\"Perform some analytics on the body\"\"\"\n",
    "\n",
    "        # Remove non-pressure records and records with bad data\n",
    "        filtered = self.filter_body(body)\n",
    "\n",
    "        # If the obs failed validation checks, skip it\n",
    "        if len(filtered) == 0:\n",
    "            self.rejected += 1\n",
    "            return\n",
    "\n",
    "        # Have we calculated the levels yet?\n",
    "        if len(self.levels) == 0:\n",
    "            # We can't calculate the levels until we've had a surface sample\n",
    "            self.levels = np.linspace(filtered[0][0], self.gph_top, self.samples)\n",
    "\n",
    "        # Convert rows to columns and interpolate to our standard levels\n",
    "        pivoted = self.body_pivot(filtered)\n",
    "\n",
    "        # Combine the results to a comma delimited list\n",
    "        out = ','.join([f\"{item:.1f}\" for item in pivoted])\n",
    "\n",
    "        # Write the record\n",
    "        self.writer.write(f'{self.hout},{out}\\n')\n",
    "\n",
    "    def filter_body(self, body: list[olieigra.BodyModel]) -> list[list[float]]:\n",
    "        \"\"\"Filter out bad data\"\"\"\n",
    "        result = [[], [], [], [], [], []]\n",
    "        usable_count = 0\n",
    "        surface_nan = 1\n",
    "        last_gph = -1\n",
    "\n",
    "        # Iterate over every body record\n",
    "        for item in body:\n",
    "            # If we have at least one record over 10k in height, we have enough data to interpolate\n",
    "            if last_gph >= 10000:\n",
    "                break\n",
    "\n",
    "            # Skip non-pressure records\n",
    "            if item.type[0] == '3':\n",
    "                continue\n",
    "\n",
    "            # Skip records with bad or missing data\n",
    "            if math.isnan(item.dpdp) | math.isnan(item.rh) | math.isnan(item.temp) | \\\n",
    "                    math.isnan(item.wdir) | math.isnan(item.wspd) | math.isnan(item.gph):\n",
    "                continue\n",
    "\n",
    "            # If we got here, the record passed validation. Add it to the results.\n",
    "            result.append(self.transform_body(item, result))\n",
    "\n",
    "            # Clear the flag if we find a valid surface sample\n",
    "            if item.type == '21':\n",
    "                surface_nan = 0\n",
    "\n",
    "            # Update tracking variables\n",
    "            last_gph = item.gph\n",
    "            usable_count += 1\n",
    "\n",
    "        # Final validation\n",
    "        if usable_count >= self.min_usable and surface_nan == 0 and last_gph >= self.gph_top:\n",
    "            # Reject the entire obs if we don't have 20 valid samples, there is \n",
    "            # not a valid surface sample, or if the balloon didn't make it to 10k\n",
    "            # above the surface.\n",
    "            return result\n",
    "        else:\n",
    "            # Passed validation, return the results\n",
    "            return []\n",
    "\n",
    "    def transform_body(self, item: olieigra.BodyModel, agg: list[list[float]]):\n",
    "        \"\"\"Transform the body\"\"\"\n",
    "\n",
    "        agg[0].append(item.gph)\n",
    "        agg[1].append(item.pres / 100.0)\n",
    "        agg[2].append(item.temp / 10.0)\n",
    "        agg[3].append((item.temp - item.dpdp) / 10.0)\n",
    "\n",
    "        # Convert wind from degrees/m^s to u,v\n",
    "        wrad = math.radians(item.wdir)\n",
    "        agg[4].append(-item.wspd * math.sin(wrad) / 10.0)\n",
    "        agg[5].append(-item.wspd * math.cos(wrad) / 10.0)\n",
    "\n",
    "    def body_pivot(self, body: list[list[float]]) -> list[float]:\n",
    "        \"\"\"Pivot and interpolate the levels\"\"\"\n",
    "        return [np.interp(level, body[0], x)\n",
    "                for level in self.levels\n",
    "                for x in [body[0], body[1], body[2], body[3], body[4], body[5]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing USM00072249-data.txt.\n",
      " Read 50909 headers, 4737531 lines. Filtered 31519. Rejected 1218. Wrote 18172 records.\n",
      "Processing USM00072250-data.txt.\n",
      " Read 78580 headers, 5759477 lines. Filtered 59498. Rejected 2026. Wrote 17056 records.\n",
      "Processing USM00072251-data.txt.\n",
      " Read 35512 headers, 5007828 lines. Filtered 16310. Rejected 2437. Wrote 16765 records.\n",
      "Processing USM00072261-data.txt.\n",
      " Read 54774 headers, 5410571 lines. Filtered 35986. Rejected 2480. Wrote 16308 records.\n",
      "Processing USM00072265-data.txt.\n",
      " Read 63936 headers, 5382956 lines. Filtered 44950. Rejected 3534. Wrote 15452 records.\n",
      "Processing USM00072357-data.txt.\n",
      " Read 27575 headers, 4371655 lines. Filtered 8706. Rejected 3201. Wrote 15668 records.\n",
      "Processing USM00072363-data.txt.\n",
      " Read 78108 headers, 5394511 lines. Filtered 59076. Rejected 3422. Wrote 15610 records.\n",
      "Processing USM00072364-data.txt.\n",
      " Read 76969 headers, 5403556 lines. Filtered 58112. Rejected 3444. Wrote 15413 records.\n",
      "Processing USM00072440-data.txt.\n",
      " Read 33986 headers, 4363593 lines. Filtered 14599. Rejected 2680. Wrote 16707 records.\n",
      "Processing USM00072451-data.txt.\n",
      " Read 66855 headers, 5124222 lines. Filtered 47852. Rejected 2266. Wrote 16737 records.\n",
      "Processing USM00072456-data.txt.\n",
      " Read 55472 headers, 5422176 lines. Filtered 36402. Rejected 2186. Wrote 16884 records.\n",
      "Processing USM00072476-data.txt.\n",
      " Read 70156 headers, 5199227 lines. Filtered 51675. Rejected 3496. Wrote 14985 records.\n",
      "Processing USM00072558-data.txt.\n",
      " Read 84756 headers, 5545662 lines. Filtered 65841. Rejected 2775. Wrote 16140 records.\n",
      "Processing USM00072562-data.txt.\n",
      " Read 77468 headers, 5447033 lines. Filtered 58738. Rejected 2914. Wrote 15816 records.\n",
      "Processing USM00072645-data.txt.\n",
      " Read 63185 headers, 5289872 lines. Filtered 44399. Rejected 2444. Wrote 16342 records.\n",
      "Processing USM00072649-data.txt.\n",
      " Read 28130 headers, 4133846 lines. Filtered 9224. Rejected 2392. Wrote 16514 records.\n",
      "Processing USM00072659-data.txt.\n",
      " Read 22598 headers, 4145048 lines. Filtered 3693. Rejected 2053. Wrote 16852 records.\n",
      "Processing USM00072662-data.txt.\n",
      " Read 70545 headers, 5356710 lines. Filtered 51686. Rejected 2435. Wrote 16424 records.\n",
      "Processing USM00072747-data.txt.\n",
      " Read 62689 headers, 5283897 lines. Filtered 44039. Rejected 2519. Wrote 16131 records.\n",
      "Processing USM00072764-data.txt.\n",
      " Read 80495 headers, 5524320 lines. Filtered 61574. Rejected 3033. Wrote 15888 records.\n",
      "Processing USM00074455-data.txt.\n",
      " Read 24614 headers, 4320985 lines. Filtered 5560. Rejected 2305. Wrote 16749 records.\n",
      "Processing USM00074560-data.txt.\n",
      " Read 22781 headers, 4253432 lines. Filtered 3539. Rejected 2815. Wrote 16427 records.\n",
      "Processing USM00074646-data.txt.\n",
      " Read 26489 headers, 2488583 lines. Filtered 0. Rejected 26489. Wrote 0 records.\n"
     ]
    }
   ],
   "source": [
    "callbacks = Gph20S10K(SILVER_GPH20S10K_PATH, datetime(2000, 1, 1))\n",
    "reader = olieigra.Reader(callbacks=callbacks)\n",
    "crawler = olieigra.Crawler(reader=reader)\n",
    "\n",
    "crawler.crawl(BRONZE_DATA_POR_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olieigra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
